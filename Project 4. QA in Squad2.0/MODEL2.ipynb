{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data - pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc((s))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_spans(text, tokenss):\n",
    "    spanss = []\n",
    "    cur_idx = 0\n",
    "    for tokens in tokenss:\n",
    "        spans = []\n",
    "        for token in tokens:\n",
    "            if text.find(token, cur_idx) < 0:\n",
    "                print(tokens)\n",
    "                print(\"{} {} {}\".format(token, cur_idx, text))\n",
    "                raise Exception()\n",
    "            cur_idx = text.find(token, cur_idx)\n",
    "            spans.append((cur_idx, cur_idx + len(token)))\n",
    "            cur_idx += len(token)\n",
    "        spanss.append(spans)\n",
    "    return spanss\n",
    "\n",
    "\n",
    "def get_word_span(context, wordss, start, stop):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    idxs = []\n",
    "    for sent_idx, spans in enumerate(spanss):\n",
    "        for word_idx, span in enumerate(spans):\n",
    "            if not (stop <= span[0] or start >= span[1]):\n",
    "                idxs.append((sent_idx, word_idx))\n",
    "    #print(spanss)\n",
    "    #print(start,stop)\n",
    "    #print(context[start:stop])\n",
    "    assert len(idxs) > 0, \"{} {} {} {}\".format(context, spanss, start, stop)\n",
    "    return idxs[0], (idxs[-1][0], idxs[-1][1] + 1)\n",
    "def get_word_idx(context, wordss, idx):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    return spanss[idx[0]][idx[1]][0]\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens1(tokens):\n",
    "   \n",
    "    \n",
    "    tokens = [w for w in tokens if w not in set(string.punctuation)]\n",
    "   # stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens(temp_tokens):\n",
    "    tokens = []\n",
    "    \n",
    "    for token in temp_tokens:\n",
    "        flag = False\n",
    "        l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\", \"/\", \"~\", '\"', \"'\", \"\\u201C\", \"\\u2019\", \"\\u201D\", \"\\u2018\", \"\\u00B0\")\n",
    "        # \\u2013 is en-dash. Used for number to nubmer\n",
    "        # l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\")\n",
    "        # l = (\"\\u2013\",)\n",
    "        tokens.extend(re.split(\"([{}])\".format(\"\".join(l)), token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def generatedataset(sentence_list,question_list):\n",
    "    s_list=[]\n",
    "    q_list=[]\n",
    "    a_list=[]\n",
    "    q_id_list=[]\n",
    "     \n",
    "    res=[]\n",
    "    #i=0\n",
    "    for q in question_list:\n",
    "        #i+=1\n",
    "        if(len(q)==0):\n",
    "            #print(i)\n",
    "            continue\n",
    "        ai=q[0][0]\n",
    "        pi=q[0][1]\n",
    "        si=q[0][2]\n",
    "        \n",
    "        for i, sent in enumerate(sentence_list[ai][pi]):\n",
    "            \n",
    "            s_list+=[' '.join(sent)]\n",
    "            q_list+=[' '.join(q[0][3])]\n",
    "            q_id_list+=[q[0][5]]\n",
    "            if i == si:\n",
    "                a_list+=[q[0][4]]\n",
    "                res+=[[q[0][3],sent,q[0][4],q[0][5]]]\n",
    "            else:\n",
    "                a_list+=[0]\n",
    "                res+=[[q[0][3],sent,0,q[0][5]]]\n",
    "            \n",
    "        \n",
    "#         res+=[[q[0][3],sentence_list[ai][pi][si],q[0][4], q[0][5]]]\n",
    "#         s_list+=[' '.join(sentence_list[ai][pi][si])]\n",
    "#         q_list+=[' '.join(q[0][3])]\n",
    "#         a_list+=[q[0][4]]\n",
    "        #for i in range(len(sentence_list[ai][pi])):\n",
    "            #if(i==si):\n",
    "                #res+=[[q[0][3],sentence_list[ai][pi][i],q[0][4], q[0][5]]]\n",
    "            #else:\n",
    "            #    res+=[[q[0][3],sentence_list[ai][pi][i],0, q[0][5]]]\n",
    "            \n",
    "    return res, s_list, q_list, a_list, q_id_list\n",
    "\n",
    "\n",
    "def create_index_dict(embed,word_counter):\n",
    "    word2idx={}\n",
    "    idx2word={}\n",
    "    i=1\n",
    "    word2idx['<UNK>']=0\n",
    "    idx2word[0]='<UNK>'\n",
    "    vocab=np.array(['<UNK>'])\n",
    "    for (key,value) in (embed.items()):\n",
    "        word2idx[key]=i\n",
    "        idx2word[i]=key\n",
    "   #     vocab=np.append(vocab,[key])\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    for key,value in (word_counter.items()):\n",
    "        key=key.lower()\n",
    "    \n",
    "        try:\n",
    "            embed[key]\n",
    "           \n",
    "        except: \n",
    "            try:\n",
    "                word2idx[key]\n",
    "            except:\n",
    "                word2idx[key]=i\n",
    "                idx2word[i]=key\n",
    "        #        vocab=np.append(vocab,[key])\n",
    "                i+=1\n",
    "   \n",
    "   # assert len(word2idx.keys())==len(vocab)\n",
    "    return word2idx,idx2word,set(word2idx.keys())\n",
    "\n",
    "def process(data):\n",
    "    q_trn=[]\n",
    "    s_trn=[]\n",
    "    y_trn=[]\n",
    "    q_id=[]\n",
    "    for i in range(len(data)):\n",
    "       # tokens = [w for w in tokens if w not in set(string.punctuation) ]\n",
    "        \n",
    "        \n",
    "        question=list(filter(None, data[i][0]))#filter empty strin\n",
    "        sentence=list(filter(None, data[i][1]))#filter empty strin\n",
    "        index=data[i][2]\n",
    "       # list1+=[[question,sentence,index]]\n",
    "        \n",
    "        q_trn+=[question]\n",
    "        s_trn+=[sentence]\n",
    "        y_trn+=[index]\n",
    "        q_id+=[data[i][3]]\n",
    "    return q_trn,s_trn,y_trn, q_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, [4, 5, 6]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "a.append([4,5,6])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type='training'\n",
    "source_path = \"{}.json\".format(data_type)\n",
    "source_data = json.load(open(source_path, 'r'))\n",
    "\n",
    "data_type='training'\n",
    "split=True\n",
    "nltk.data.path.append(\"/afs/cs.stanford.edu/u/tianzhao/tz/nltk_data\")\n",
    "sent_tokenize = nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/afs/cs.stanford.edu/u/tianzhao/tz/nltk_data\")\n",
    "sent_tokenize = nltk.sent_tokenize\n",
    "word_counter = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:47<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "context_list=[]\n",
    "sentence_list=[]\n",
    "question_list=[]\n",
    "answer_list=[]\n",
    "answer_index=[]\n",
    "label_index=[]\n",
    "label=[]\n",
    "idxs=[]\n",
    "\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "for ai, article in enumerate(tqdm(source_data['data'])):\n",
    "    s=[]\n",
    "    p = []\n",
    "    for pi, para in enumerate(article['paragraphs']):\n",
    "        # wordss\n",
    "        context = para['context']\n",
    "        context = context.replace(\"''\", '\" ')\n",
    "        context = context.replace(\"``\", '\" ')\n",
    "       \n",
    "        si = list(map(word_tokenize, sent_tokenize(context)))\n",
    "       # print(si)\n",
    "        si = [process_tokens(tokens) for tokens in si]  # process tokens\n",
    "        for sentence in si:\n",
    "            for word in sentence:\n",
    "                 word_counter[word] += 1\n",
    " \n",
    "        # given xi, add chars\n",
    "        s.append(si)\n",
    "        p.append(context)\n",
    "\n",
    "        indexi = [ai, pi]\n",
    "        for qa in para['qas']:\n",
    "                qi = word_tokenize(qa['question'])\n",
    "                if qa['is_impossible']:\n",
    "                    labeli=[0]\n",
    "                    #print(labeli)\n",
    "                else:\n",
    "                    labeli=[1]       \n",
    "                yi = []\n",
    "                yyi=[] \n",
    "                answers = []\n",
    "                q_id = qa['id']\n",
    "                \n",
    "                ans=[]\n",
    "                if labeli[0] == 1:\n",
    "                    ans=qa['answers']\n",
    "                else:\n",
    "                    ans=qa['plausible_answers']\n",
    "                    \n",
    "                for answer in ans:\n",
    "                    answer_text=answer['text']\n",
    "                    answers.append(answer_text)\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_stop = answer_start + len(answer_text)\n",
    "                   # print(context)\n",
    "                   # print(si)\n",
    "                    yi0, yi1 = get_word_span(context, si, answer_start, answer_stop)\n",
    "                    #w0 = xi[yi0[0]][yi0[1]]            \n",
    "                    #w1 = xi[yi1[0]][yi1[1]-1]\n",
    "                    yi.append([ai,pi,yi0[0],qi,labeli[0],q_id]) \n",
    "                    #if labeli[0] == 0:\n",
    "                        #print(yi)\n",
    "                   # print(yi)\n",
    "                   # print(yi)\n",
    "                    yyi.append([answer_start,answer_stop])\n",
    "                for qij in qi:            \n",
    "                         word_counter[qij] += 1\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                question_list.append(qi)\n",
    "                answer_list.append(answers)\n",
    "                answer_index.append(yi)\n",
    "                label.append(labeli)\n",
    "                label_index.append(indexi)\n",
    "               # ids.append(qa['id'])\n",
    "                idxs.append(len(idxs))\n",
    "         \n",
    "    sentence_list.append(s)\n",
    "\n",
    "    context_list.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, s_list, q_list, a_list, q_id_list =generatedataset(sentence_list,answer_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34798"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr=0\n",
    "for l in answer_index:\n",
    "    try:\n",
    "        if l[0][4] ==0:\n",
    "            ctr+=1\n",
    "    except:\n",
    "        continue\n",
    "ctr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352635\n",
      "352635\n",
      "352635\n",
      "352635\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(s_list))\n",
    "print(len(q_list))\n",
    "print(len(a_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beyoncé Giselle Knowles - Carter (  / biːˈjɒnseɪ /  bee - YON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress .',\n",
       " \"Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny  ' s Child .\",\n",
       " \"Managed by her father , Mathew Knowles , the group became one of the world  ' s best - selling girl groups of all time .\",\n",
       " 'Their hiatus saw the release of Beyoncé  \\' s debut album , Dangerously in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles  \"  Crazy in Love  \"  and  \"  Baby Boy  \"  .',\n",
       " 'Beyoncé Giselle Knowles - Carter (  / biːˈjɒnseɪ /  bee - YON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress .',\n",
       " \"Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny  ' s Child .\",\n",
       " \"Managed by her father , Mathew Knowles , the group became one of the world  ' s best - selling girl groups of all time .\",\n",
       " 'Their hiatus saw the release of Beyoncé  \\' s debut album , Dangerously in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles  \"  Crazy in Love  \"  and  \"  Baby Boy  \"  .',\n",
       " 'Beyoncé Giselle Knowles - Carter (  / biːˈjɒnseɪ /  bee - YON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress .',\n",
       " \"Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny  ' s Child .\",\n",
       " \"Managed by her father , Mathew Knowles , the group became one of the world  ' s best - selling girl groups of all time .\",\n",
       " 'Their hiatus saw the release of Beyoncé  \\' s debut album , Dangerously in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles  \"  Crazy in Love  \"  and  \"  Baby Boy  \"  .',\n",
       " 'Beyoncé Giselle Knowles - Carter (  / biːˈjɒnseɪ /  bee - YON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress .',\n",
       " \"Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny  ' s Child .\",\n",
       " \"Managed by her father , Mathew Knowles , the group became one of the world  ' s best - selling girl groups of all time .\",\n",
       " 'Their hiatus saw the release of Beyoncé  \\' s debut album , Dangerously in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles  \"  Crazy in Love  \"  and  \"  Baby Boy  \"  .',\n",
       " 'Beyoncé Giselle Knowles - Carter (  / biːˈjɒnseɪ /  bee - YON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress .',\n",
       " \"Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny  ' s Child .\",\n",
       " \"Managed by her father , Mathew Knowles , the group became one of the world  ' s best - selling girl groups of all time .\",\n",
       " 'Their hiatus saw the release of Beyoncé  \\' s debut album , Dangerously in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles  \"  Crazy in Love  \"  and  \"  Baby Boy  \"  .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 1000000\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"./encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model\n",
    "\n",
    "\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = 'glove.840B.300d.txt' if model_version == 1 else '../dataset/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_embedding(sent_list):\n",
    "    embeddings = model.encode(sent_list, bsize=128, tokenize=False, verbose=True)\n",
    "    return embeddings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 4550040/4582850 (99.3%)\n",
      "Speed : 146.6 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 10492107/10573990 (99.2%)\n",
      "Speed : 60.1 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "q_embed = get_sent_embedding(q_list)\n",
    "s_embed = get_sent_embedding(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('q_embed_2', q_embed)\n",
    "np.save('s_embed_2', s_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('a_list_2', a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embed = np.load('q_embed_2.npy')\n",
    "s_embed = np.load('s_embed_2.npy')\n",
    "a_list = np.load('a_list_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352635, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352635, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity - Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_loop(q_embed, s_embed):\n",
    "    sim = []\n",
    "    for i in range(q_embed.shape[0]):\n",
    "        prod = q_embed[i,:].dot(s_embed[i,:])\n",
    "        prod /= np.sqrt((q_embed[i,:] ** 2).sum())\n",
    "        prod /= np.sqrt((s_embed[i,:] ** 2).sum())\n",
    "        sim+=[prod]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cos_sim_loop(q_embed, s_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08217849"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentence based on max sim\n",
    "q_id_max = {}\n",
    "for i in range(len(q_id_list)):\n",
    "    #find max sim score for q_id\n",
    "      \n",
    "    if q_id_list[i] in q_id_max.keys():\n",
    "        if cos_sim[i] > cos_sim[q_id_max[q_id_list[i]]]:\n",
    "            q_id_max[q_id_list[i]] = i\n",
    "    else:\n",
    "        q_id_max[q_id_list[i]] = i\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2keep = sorted(list(q_id_max.values()))\n",
    "new_q_id_list = [q_id_list[i] for i in ind2keep]\n",
    "new_q_list = [q_list[i] for i in ind2keep]\n",
    "new_a_list = [a_list[i] for i in ind2keep]\n",
    "new_s_list = [s_list[i] for i in ind2keep]\n",
    "\n",
    "new_q_embed =  q_embed[ind2keep[:],:]\n",
    "new_s_embed =  s_embed[ind2keep[:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineimilarity(e,v):\n",
    "  \n",
    "    #Input:\n",
    "    #e = nxd input matrix with n row-vectors of dimensionality d (n is number of dictionary_keys)\n",
    "    #v = mxd input matrix with m row-vectors of dimensionality d (m is number of test samples)\n",
    "    # Output:\n",
    "    # Matrix D of size nxm\n",
    "    # s(i,j) is the cosinesimiarlity of embed(i,:) and test(j,:)\n",
    "    g=e.dot(v.T)\n",
    "    b=np.expand_dims(np.linalg.norm(e,axis=1),1)\n",
    "    a=np.expand_dims(np.linalg.norm(v,axis=1),1)\n",
    "    s=np.divide(g,np.multiply(b,a.T))\n",
    "    # ... until here\n",
    "    return s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosineimilarity(q_embed,s_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_cos_sim(sim_mat, threshold=0.6):\n",
    "    preds=[]\n",
    "    \n",
    "    sim = sim_mat.diagonal()\n",
    "    \n",
    "    for i in range(sim.shape[0]):\n",
    "        \n",
    "        if abs(sim[i])>threshold:\n",
    "            preds+=[1]\n",
    "        else:\n",
    "            preds+=[0]\n",
    "    return sim,preds\n",
    "\n",
    "def get_acc(preds,gold_labels):\n",
    "    correct=0;\n",
    "    total=len(preds)\n",
    "    for i in range(total):\n",
    "        if preds[i]==gold_labels[i]:\n",
    "            correct+=1\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim, preds = get_pred_cos_sim(sim_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess_syntactic(sent_list):\n",
    "    result=[]\n",
    "    for sent in sent_list:\n",
    "        filtered_sentence = []\n",
    "        word_tokens = word_tokenize(sent) \n",
    "        filtered_sentence = [stemmer.stem(w) for w in word_tokens if not w in stop_words] \n",
    "        result+=[filtered_sentence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list_syn = preprocess_syntactic(q_list)\n",
    "s_list_syn = preprocess_syntactic(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syntactic_non_similarity_score(s_list_syn, q_list_syn):\n",
    "    \n",
    "    result=[]\n",
    "    \n",
    "    for i,q in enumerate(q_list_syn):\n",
    "         \n",
    "        ctr = 0\n",
    "        \n",
    "        for w in q:\n",
    "            if w in list(set(s_list_syn[i])):\n",
    "                ctr+=1\n",
    "        result+=[ ctr / len(q) if len(q)>0 else 0]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_sim = get_syntactic_non_similarity_score(s_list_syn, q_list_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.625,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42857142857142855,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.4444444444444444,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.5555555555555556,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.2857142857142857,\n",
       " 0.42857142857142855,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.5714285714285714,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.18181818181818182,\n",
       " 0.45454545454545453,\n",
       " 0.0,\n",
       " 0.18181818181818182,\n",
       " 0.18181818181818182,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.1111111111111111,\n",
       " 0.2222222222222222,\n",
       " 0.3333333333333333,\n",
       " 0.1111111111111111,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.5555555555555556,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.4444444444444444,\n",
       " 0.1111111111111111,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.1111111111111111,\n",
       " 0.3333333333333333,\n",
       " 0.4444444444444444,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.6,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.42857142857142855,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1875,\n",
       " 0.25,\n",
       " 0.0625,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.1875,\n",
       " 0.18181818181818182,\n",
       " 0.36363636363636365,\n",
       " 0.18181818181818182,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.2727272727272727,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09090909090909091,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.3,\n",
       " 0.4,\n",
       " 0.1,\n",
       " 0.4,\n",
       " 0.3,\n",
       " 0.1,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.08333333333333333,\n",
       " 0.0,\n",
       " 0.4166666666666667,\n",
       " 0.0,\n",
       " 0.08333333333333333,\n",
       " 0.75,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.08333333333333333,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.4444444444444444,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.18181818181818182,\n",
       " 0.18181818181818182,\n",
       " 0.09090909090909091,\n",
       " 0.36363636363636365,\n",
       " 0.18181818181818182,\n",
       " 0.45454545454545453,\n",
       " 0.09090909090909091,\n",
       " 0.45454545454545453,\n",
       " 0.36363636363636365,\n",
       " 0.2727272727272727,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.5,\n",
       " 0.36363636363636365,\n",
       " 0.2727272727272727,\n",
       " 0.18181818181818182,\n",
       " 0.2727272727272727,\n",
       " 0.2727272727272727,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.5833333333333334,\n",
       " 0.16666666666666666,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.2222222222222222,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.2,\n",
       " 0.6,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.2222222222222222,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.2857142857142857,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.08333333333333333,\n",
       " 0.25,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.25,\n",
       " 0.4166666666666667,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.625,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.5,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.4,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.4166666666666667,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5555555555555556,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42857142857142855,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.3,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 0.4,\n",
       " 0.2,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.13333333333333333,\n",
       " 0.2,\n",
       " 0.26666666666666666,\n",
       " 0.4666666666666667,\n",
       " 0.13333333333333333,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.2857142857142857,\n",
       " 0.14285714285714285,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.08333333333333333,\n",
       " 0.5833333333333334,\n",
       " 0.5,\n",
       " 0.5833333333333334,\n",
       " 0.3333333333333333,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.08333333333333333,\n",
       " 0.08333333333333333,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.375,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.5,\n",
       " 0.125,\n",
       " 0.5555555555555556,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4444444444444444,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.2222222222222222,\n",
       " 0.1111111111111111,\n",
       " 0.4444444444444444,\n",
       " 0.2222222222222222,\n",
       " 0.1111111111111111,\n",
       " 0.375,\n",
       " 0.625,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.36363636363636365,\n",
       " 0.09090909090909091,\n",
       " 0.36363636363636365,\n",
       " 0.2727272727272727,\n",
       " 0.2727272727272727,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2222222222222222,\n",
       " 0.1111111111111111,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.4444444444444444,\n",
       " 0.1111111111111111,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.125,\n",
       " 0.2857142857142857,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.14285714285714285,\n",
       " 0.7142857142857143,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.14285714285714285,\n",
       " 0.2857142857142857,\n",
       " 0.5,\n",
       " 0.3333333333333333,\n",
       " 0.4,\n",
       " 0.2,\n",
       " 0.25,\n",
       " 0.625,\n",
       " 0.3333333333333333,\n",
       " 0.4444444444444444,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7272727272727273,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2727272727272727,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.36363636363636365,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.625,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.3,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.4444444444444444,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.2,\n",
       " 0.5,\n",
       " 0.3,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2727272727272727,\n",
       " 0.2727272727272727,\n",
       " 0.09090909090909091,\n",
       " 0.6363636363636364,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.2857142857142857,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.16666666666666666,\n",
       " 0.2857142857142857,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42857142857142855,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.42857142857142855,\n",
       " 0.5555555555555556,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.2222222222222222,\n",
       " 0.1111111111111111,\n",
       " 0.1111111111111111,\n",
       " 0.6,\n",
       " 0.75,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.25,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 0.08333333333333333,\n",
       " 0.08333333333333333,\n",
       " 0.0,\n",
       " 0.4166666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.4,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.7142857142857143,\n",
       " 0.2857142857142857,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.625,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.6,\n",
       " 0.4444444444444444,\n",
       " 0.5555555555555556,\n",
       " 0.3333333333333333,\n",
       " 0.2222222222222222,\n",
       " 0.3333333333333333,\n",
       " 0.1111111111111111,\n",
       " 0.2222222222222222,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5555555555555556,\n",
       " 0.2222222222222222,\n",
       " 0.2857142857142857,\n",
       " 0.2857142857142857,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.42857142857142855,\n",
       " 0.5714285714285714,\n",
       " 0.16666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.2222222222222222,\n",
       " 0.3333333333333333,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.1,\n",
       " 0.6,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.1,\n",
       " 0.4,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.375,\n",
       " 0.75,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentence based on max syn sim\n",
    "q_id_max_syn = {}\n",
    "for i in range(len(q_id_list)):\n",
    "    #find max sim score for q_id\n",
    "      \n",
    "    if q_id_list[i] in q_id_max_syn.keys():\n",
    "        if syn_sim[i] > syn_sim[q_id_max_syn[q_id_list[i]]]:\n",
    "            q_id_max_syn[q_id_list[i]] = i\n",
    "    else:\n",
    "        q_id_max_syn[q_id_list[i]] = i\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2keep_syn = sorted(list(q_id_max_syn.values()))\n",
    "new_q_id_list_syn = [q_id_list[i] for i in ind2keep_syn]\n",
    "new_q_list_syn = [q_list[i] for i in ind2keep_syn]\n",
    "new_a_list_syn = [a_list[i] for i in ind2keep_syn]\n",
    "new_s_list_syn = [s_list[i] for i in ind2keep_syn]\n",
    "\n",
    "new_q_embed_syn =  q_embed[ind2keep_syn[:],:]\n",
    "new_s_embed_syn =  s_embed[ind2keep_syn[:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cos_sim = [cos_sim[i] for i in ind2keep]\n",
    "new_syn_sim = [syn_sim[i] for i in ind2keep_syn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69596\n",
      "69596\n"
     ]
    }
   ],
   "source": [
    "print(len(new_syn_sim))\n",
    "print(len(new_syn_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cos_sim = np.array(new_cos_sim).reshape(-1,1)\n",
    "new_syn_sim = np.array(new_syn_sim).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr = np.concatenate([new_q_embed, new_s_embed , new_s_embed_syn, new_cos_sim, new_syn_sim], axis=1)\n",
    "yTr = max(new_a_list_syn, new_a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(xTr, yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:39<00:00, 11.29it/s]\n"
     ]
    }
   ],
   "source": [
    "te_data_type='development'\n",
    "te_source_path = \"{}.json\".format(te_data_type)\n",
    "te_source_data = json.load(open(te_source_path, 'r'))\n",
    "\n",
    "\n",
    "\n",
    "te_context_list=[]\n",
    "te_sentence_list=[]\n",
    "te_question_list=[]\n",
    "te_answer_list=[]\n",
    "te_answer_index=[]\n",
    "te_label_index=[]\n",
    "te_label=[]\n",
    "te_idxs=[]\n",
    "te_word_counter = Counter()\n",
    "\n",
    "for ai, article in enumerate(tqdm(te_source_data['data'])):\n",
    "    s=[]\n",
    "    p = []\n",
    "    for pi, para in enumerate(article['paragraphs']):\n",
    "        # wordss\n",
    "        context = para['context']\n",
    "        context = context.replace(\"''\", '\" ')\n",
    "        context = context.replace(\"``\", '\" ')\n",
    "       \n",
    "        si = list(map(word_tokenize, sent_tokenize(context)))\n",
    "       # print(si)\n",
    "        si = [process_tokens(tokens) for tokens in si]  # process tokens\n",
    "        for sentence in si:\n",
    "            for word in sentence:\n",
    "                 word_counter[word] += 1\n",
    " \n",
    "        # given xi, add chars\n",
    "        s.append(si)\n",
    "        p.append(context)\n",
    "\n",
    "        indexi = [ai, pi]\n",
    "        for qa in para['qas']:\n",
    "                qi = word_tokenize(qa['question'])\n",
    "                \n",
    "                if qa['is_impossible']:\n",
    "                    labeli=[0]\n",
    "                  #  print(labeli)\n",
    "                else:\n",
    "                    labeli=[1] \n",
    "                       \n",
    "                yi = []\n",
    "                yyi=[] \n",
    "                answers = []\n",
    "                q_id = qa['id']\n",
    "                \n",
    "                ans=[]\n",
    "                if labeli[0] == 1:\n",
    "                    ans=qa['answers']\n",
    "                else:\n",
    "                    ans=qa['plausible_answers']\n",
    "                    \n",
    "                \n",
    "                for answer in ans:\n",
    "                    answer_text=answer['text']\n",
    "                    answers.append(answer_text)\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_stop = answer_start + len(answer_text)\n",
    "                   # print(context)\n",
    "                   # print(si)\n",
    "                    yi0, yi1 = get_word_span(context, si, answer_start, answer_stop)\n",
    "                    #w0 = xi[yi0[0]][yi0[1]]            \n",
    "                    #w1 = xi[yi1[0]][yi1[1]-1]\n",
    "                    yi.append([ai,pi,yi0[0],qi,labeli[0],q_id]) \n",
    "                   # print(yi)\n",
    "                   # print(yi)\n",
    "                    yyi.append([answer_start,answer_stop])\n",
    "                for qij in qi:\n",
    "                    \n",
    "                            word_counter[qij] += 1\n",
    "                te_question_list.append(qi)\n",
    "                te_answer_list.append(answers)\n",
    "                te_answer_index.append(yi)\n",
    "                te_label.append(labeli)\n",
    "                te_label_index.append(indexi)\n",
    "               # ids.append(qa['id'])\n",
    "                te_idxs.append(len(idxs))\n",
    "         \n",
    "    te_sentence_list.append(s)\n",
    "\n",
    "    te_context_list.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X, te_s_list, te_q_list, te_a_list, te_q_id_list=generatedataset(te_sentence_list,te_answer_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53971"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(te_s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5a7e070b70df9f001a87543a'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17400\n",
      "17400\n",
      "17400\n",
      "17400\n"
     ]
    }
   ],
   "source": [
    "print(len(te_X))\n",
    "print(len(te_s_list))\n",
    "print(len(te_q_list))\n",
    "print(len(te_a_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8700, 1: 8700})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(te_a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 1140849/1149398 (99.3%)\n",
      "Speed : 141.2 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2625053/2645250 (99.2%)\n",
      "Speed : 64.5 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "te_q_embed = get_sent_embedding(te_q_list)\n",
    "te_s_embed = get_sent_embedding(te_s_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('te_q_embed_2',te_q_embed)\n",
    "np.save('te_s_embed_2',te_s_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_q_embed = np.load('te_q_embed_2.npy')\n",
    "te_s_embed = np.load('te_s_embed_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88248, 4096)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_s_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_cos_sim = cos_sim_loop(te_q_embed, te_s_embed)\n",
    "#get sentence based on max sim\n",
    "te_q_id_max = {}\n",
    "for i in range(len(te_q_id_list)):\n",
    "    #find max sim score for q_id\n",
    "      \n",
    "    if te_q_id_list[i] in te_q_id_max.keys():\n",
    "        if te_cos_sim[i] > te_cos_sim[te_q_id_max[te_q_id_list[i]]]:\n",
    "            te_q_id_max[te_q_id_list[i]] = i\n",
    "    else:\n",
    "        te_q_id_max[te_q_id_list[i]] = i\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "te_ind2keep = sorted(list(te_q_id_max.values()))\n",
    "te_new_q_id_list = [te_q_id_list[i] for i in te_ind2keep]\n",
    "te_new_q_list = [te_q_list[i] for i in te_ind2keep]\n",
    "te_new_a_list = [te_a_list[i] for i in te_ind2keep]\n",
    "te_new_s_list = [te_s_list[i] for i in te_ind2keep]\n",
    "\n",
    "te_new_q_embed =  te_q_embed[te_ind2keep[:],:]\n",
    "te_new_s_embed =  te_s_embed[te_ind2keep[:],:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "te_q_list_syn = preprocess_syntactic(te_q_list)\n",
    "te_s_list_syn = preprocess_syntactic(te_s_list)\n",
    "\n",
    "\n",
    "te_syn_sim = get_syntactic_non_similarity_score(te_s_list_syn, te_q_list_syn)\n",
    "\n",
    "#get sentence based on max syn sim\n",
    "te_q_id_max_syn = {}\n",
    "for i in range(len(te_q_id_list)):\n",
    "    #find max sim score for q_id\n",
    "      \n",
    "    if te_q_id_list[i] in te_q_id_max_syn.keys():\n",
    "        if te_syn_sim[i] > te_syn_sim[te_q_id_max_syn[te_q_id_list[i]]]:\n",
    "            te_q_id_max_syn[te_q_id_list[i]] = i\n",
    "    else:\n",
    "        te_q_id_max_syn[te_q_id_list[i]] = i\n",
    "  \n",
    "\n",
    "te_ind2keep_syn = sorted(list(te_q_id_max_syn.values()))\n",
    "te_new_q_id_list_syn = [te_q_id_list[i] for i in te_ind2keep_syn]\n",
    "te_new_q_list_syn = [te_q_list[i] for i in te_ind2keep_syn]\n",
    "te_new_a_list_syn = [te_a_list[i] for i in te_ind2keep_syn]\n",
    "te_new_s_list_syn = [te_s_list[i] for i in te_ind2keep_syn]\n",
    "\n",
    "te_new_q_embed_syn =  te_q_embed[te_ind2keep_syn[:],:]\n",
    "te_new_s_embed_syn =  te_s_embed[te_ind2keep_syn[:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_new_cos_sim = [te_cos_sim[i] for i in te_ind2keep]\n",
    "te_new_syn_sim = [te_syn_sim[i] for i in te_ind2keep_syn]\n",
    "\n",
    "te_new_cos_sim = np.array(te_new_cos_sim).reshape(-1,1)\n",
    "te_new_syn_sim = np.array(te_new_syn_sim).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTe = np.concatenate([te_new_q_embed, te_new_s_embed , te_new_s_embed_syn, te_new_cos_sim, te_new_syn_sim], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTe = clf.predict(xTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5753204207138342"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc(yTr,a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = get_prediction_dict(yTe, te_new_q_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result('model2-dev.csv',pred_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pred_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(true_positives, false_positives):\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def calculate_recall(true_positives, false_negatives):\n",
    "    if true_positives + false_negatives == 0:\n",
    "        return 0\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_accuracy(true_positive, true_negative, false_positive, false_negative):\n",
    "    return (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(pred, gold_labels):\n",
    "    \"\"\"\n",
    "\n",
    "    :param preds: A dictionary mapping question IDs to:\n",
    "                    -> 0 if the question does not have a plausible answer\n",
    "                    -> 1 if the question does have a plausible answer\n",
    "\n",
    "    :param test_set: A dictionary of the following form:\n",
    "\n",
    "    test_set[\"data\"] -> list of titles\n",
    "        title[\"title\"] -> title name\n",
    "        title[\"paragraphs\"] -> list of paragraphs\n",
    "            paragraph[\"context\"] -> data which might answer questions in this paragraph\n",
    "            paragraph[\"qas\"] -> list of questions\n",
    "                question[\"question\"] -> question content\n",
    "                question[\"id\"] -> question ID\n",
    "\n",
    "    :return: A 3-tuple consisting of (precision, recall, f1_score)\n",
    "    \"\"\"\n",
    "\n",
    "#     correct_mapping = {}\n",
    "\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "#     for title in test_set[\"data\"]:\n",
    "#         for paragraph in title[\"paragraphs\"]:\n",
    "#             for question in paragraph[\"qas\"]:\n",
    "#                 if question[\"is_impossible\"]:\n",
    "#                     correct_mapping[question[\"id\"]] = 0\n",
    "#                 else:\n",
    "#                     correct_mapping[question[\"id\"]] = 1\n",
    "\n",
    "#     for id, pred in correct_mapping.items():\n",
    "#         if pred == 0 and preds[id] == 0:\n",
    "#             true_negatives += 1\n",
    "#         elif pred == 1 and preds[id] == 1:\n",
    "#             true_positives += 1\n",
    "#         elif pred == 0 and preds[id] == 1:\n",
    "#             false_positives += 1\n",
    "#         elif pred == 1 and preds[id] == 0:\n",
    "#             false_negatives += 1\n",
    "     \n",
    "    if len(pred) != len(gold_labels):\n",
    "        print(\"Error\")\n",
    "        return\n",
    "            \n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == 0 and gold_labels[i] == 0:\n",
    "            true_negatives += 1\n",
    "        elif pred[i] == 1 and gold_labels[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif pred[i] == 0 and gold_labels[i] == 1:\n",
    "            false_positives += 1\n",
    "        elif pred[i] == 1 and gold_labels[i] == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "    print(\"Intermediate evaluation:\")\n",
    "    print(\"true_positives: \" + str(true_positives))\n",
    "    print(\"true_negatives: \" + str(true_negatives))\n",
    "    print(\"false_positives: \" + str(false_positives))\n",
    "    print(\"false_negatives: \" + str(false_negatives))\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "    precision = calculate_precision(true_positives, false_positives)\n",
    "    recall = calculate_recall(true_positives, false_negatives)\n",
    "    f_1 = calculate_f1(precision, recall)\n",
    "    accuracy = calculate_accuracy(true_positives, true_negatives, false_positives, false_negatives)\n",
    "    return precision, recall, f_1, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, test_set):\n",
    "    \"\"\"\n",
    "\n",
    "    :param preds: A dictionary mapping question IDs to:\n",
    "                    -> 0 if the question does not have a plausible answer\n",
    "                    -> 1 if the question does have a plausible answer\n",
    "\n",
    "    :param test_set: A dictionary of the following form:\n",
    "\n",
    "    test_set[\"data\"] -> list of titles\n",
    "        title[\"title\"] -> title name\n",
    "        title[\"paragraphs\"] -> list of paragraphs\n",
    "            paragraph[\"context\"] -> data which might answer questions in this paragraph\n",
    "            paragraph[\"qas\"] -> list of questions\n",
    "                question[\"question\"] -> question content\n",
    "                question[\"id\"] -> question ID\n",
    "\n",
    "    :return: A 3-tuple consisting of (precision, recall, f1_score)\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    n=len(pred)\n",
    "    for i in range(n):\n",
    "        if pred[i] == 0 and test_set[i] == 0:\n",
    "            true_negatives += 1\n",
    "        elif pred[i] == 1 and test_set[i]  == 1:\n",
    "            true_positives += 1\n",
    "        elif pred[i]== 0 and test_set[i] == 1:\n",
    "            false_positives += 1\n",
    "        elif pred[i] == 1 and test_set[i] == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "    precision = calculate_precision(true_positives, false_positives)\n",
    "    recall = calculate_recall(true_positives, false_negatives)\n",
    "    f_1 = calculate_f1(precision, recall)\n",
    "    acc=(true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
    "    return precision, recall, f_1,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate evaluation:\n",
      "true_positives: 65\n",
      "true_negatives: 15078\n",
      "false_positives: 2101\n",
      "false_negatives: 156\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.030009233610341645,\n",
       " 0.29411764705882354,\n",
       " 0.054461667364893177,\n",
       " 0.8702873563218391)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(yTe, te_new_a_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17400"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17400"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(te_new_a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:09<00:00,  3.54it/s]\n"
     ]
    }
   ],
   "source": [
    "te_data_type='test'\n",
    "te_source_path = \"{}.json\".format(te_data_type)\n",
    "te_source_data = json.load(open(te_source_path, 'r'))\n",
    "\n",
    "test_q=[]\n",
    "test_q_id=[]\n",
    "test_sen=[]\n",
    "\n",
    "for ai, article in enumerate(tqdm(te_source_data['data'])):\n",
    "    s=[]\n",
    "    p = []\n",
    "    for pi, para in enumerate(article['paragraphs']):\n",
    "        # wordss\n",
    "        context = para['context']\n",
    "        context = context.replace(\"''\", '\" ')\n",
    "        context = context.replace(\"``\", '\" ')\n",
    "       \n",
    "        si = list(map(word_tokenize, sent_tokenize(context)))\n",
    "       # print(si)\n",
    "        si = [process_tokens(tokens) for tokens in si]  # process tokens\n",
    "        for sentence in si:\n",
    "            for word in sentence:\n",
    "                 word_counter[word] += 1\n",
    " \n",
    "\n",
    "\n",
    "            for qa in para['qas']:\n",
    "                    qi = word_tokenize(qa['question'])\n",
    "                    q_id = qa['id']\n",
    "\n",
    "                    test_q.append(' '.join(qi))\n",
    "                    test_q_id.append(q_id)\n",
    "                    test_sen.append(' '.join(sentence))\n",
    "\n",
    "\n",
    "               \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q_id_list = test_q_id\n",
    "test_s_list =  test_sen\n",
    "test_q_list = test_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 836633/843857 (99.1%)\n",
      "Speed : 144.9 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 1890676/1907098 (99.1%)\n",
      "Speed : 67.8 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "test_q_embed = get_sent_embedding(test_q)\n",
    "test_s_embed = get_sent_embedding(test_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_q_embed', test_q_embed)\n",
    "np.save('test_s_embed', test_s_embed)\n",
    "np.save('test_q_id',test_q_id)\n",
    "\n",
    "np.save('te_q_embed', te_q_embed)\n",
    "np.save('te_s_embed', te_s_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('te_a_list', te_a_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q_embed = np.load('test_q_embed.npy')\n",
    "test_s_embed = np.load('test_s_embed.npy')\n",
    "test_q_id_list = np.load('test_q_id.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_sim = cos_sim_loop(test_q_embed, test_s_embed)\n",
    "#get sentence based on max sim\n",
    "test_q_id_max = {}\n",
    "for i in range(len(test_q_id_list)):\n",
    "    #find max sim score for q_id\n",
    "      \n",
    "    if test_q_id_list[i] in test_q_id_max.keys():\n",
    "        if test_cos_sim[i] > test_cos_sim[test_q_id_max[test_q_id_list[i]]]:\n",
    "            test_q_id_max[test_q_id_list[i]] = i\n",
    "    else:\n",
    "        test_q_id_max[test_q_id_list[i]] = i\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "test_ind2keep = sorted(list(test_q_id_max.values()))\n",
    "test_new_q_id_list = [test_q_id_list[i] for i in test_ind2keep]\n",
    "# test_new_q_list = [test_q_list[i] for i in test_ind2keep]\n",
    "# test_new_a_list = [test_a_list[i] for i in test_ind2keep]\n",
    "# test_new_s_list = [test_s_list[i] for i in test_ind2keep]\n",
    "\n",
    "test_new_q_embed =  test_q_embed[test_ind2keep[:],:]\n",
    "test_new_s_embed =  test_s_embed[test_ind2keep[:],:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_q_list_syn = preprocess_syntactic(test_q_list)\n",
    "test_s_list_syn = preprocess_syntactic(test_s_list)\n",
    "\n",
    "\n",
    "test_syn_sim = get_syntactic_non_similarity_score(test_s_list_syn, test_q_list_syn)\n",
    "\n",
    "#get sentence based on max syn sim\n",
    "test_q_id_max_syn = {}\n",
    "for i in range(len(test_q_id_list)):\n",
    "    #find max sim score for q_id\n",
    "      \n",
    "    if test_q_id_list[i] in test_q_id_max_syn.keys():\n",
    "        if test_syn_sim[i] > test_syn_sim[test_q_id_max_syn[test_q_id_list[i]]]:\n",
    "            test_q_id_max_syn[test_q_id_list[i]] = i\n",
    "    else:\n",
    "        test_q_id_max_syn[test_q_id_list[i]] = i\n",
    "  \n",
    "\n",
    "test_ind2keep_syn = sorted(list(test_q_id_max_syn.values()))\n",
    "test_new_q_id_list_syn = [test_q_id_list[i] for i in test_ind2keep_syn]\n",
    "# test_new_q_list_syn = [test_q_list[i] for i in test_ind2keep_syn]\n",
    "# test_new_a_list_syn = [test_a_list[i] for i in test_ind2keep_syn]\n",
    "# test_new_s_list_syn = [test_s_list[i] for i in test_ind2keep_syn]\n",
    "\n",
    "test_new_q_embed_syn =  test_q_embed[test_ind2keep_syn[:],:]\n",
    "test_new_s_embed_syn =  test_s_embed[test_ind2keep_syn[:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_cos_sim = [test_cos_sim[i] for i in test_ind2keep]\n",
    "test_new_syn_sim = [test_syn_sim[i] for i in test_ind2keep_syn]\n",
    "\n",
    "test_new_cos_sim = np.array(test_new_cos_sim).reshape(-1,1)\n",
    "test_new_syn_sim = np.array(test_new_syn_sim).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest = np.concatenate([test_new_q_embed, test_new_s_embed , test_new_s_embed_syn, test_new_cos_sim, test_new_syn_sim], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTest = clf.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_dict(preds, q_ids):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_result(filename,preds):\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['Id','Category'])\n",
    "        for k,v in preds.items():\n",
    "            filewriter.writerow([k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = get_prediction_dict(yTest, test_new_q_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result('lr2.csv',test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11856"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = test_preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pred={}\n",
    "for k in keys:\n",
    "    one_pred[k]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_result('one.csv',one_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrCombined = np.concatenate([xTr,xTe], axis=0)\n",
    "yTrCombined = np.concatenate([yTr,yTe], axis=0)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(xTrCombined, yTrCombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrCombined_pred = clf.predict(xTrCombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acc(yTrCombined_pred, yTrCombined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
