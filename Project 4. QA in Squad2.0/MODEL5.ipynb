{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,GRU, Bidirectional, TimeDistributed, GRU, AveragePooling1D, Reshape, GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import  Bidirectional,Input,Dropout\n",
    "from keras.layers import Input, Dense, Dropout, Reshape, Flatten,merge,concatenate\n",
    "from keras import Model\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import multi_gpu_model\n",
    "from rnnlayer import Attention, SimpleAttention\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical#, accuracy\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed, Recurrent\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc((s))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_spans(text, tokenss):\n",
    "    spanss = []\n",
    "    cur_idx = 0\n",
    "    for tokens in tokenss:\n",
    "        spans = []\n",
    "        for token in tokens:\n",
    "            if text.find(token, cur_idx) < 0:\n",
    "                print(tokens)\n",
    "                print(\"{} {} {}\".format(token, cur_idx, text))\n",
    "                raise Exception()\n",
    "            cur_idx = text.find(token, cur_idx)\n",
    "            spans.append((cur_idx, cur_idx + len(token)))\n",
    "            cur_idx += len(token)\n",
    "        spanss.append(spans)\n",
    "    return spanss\n",
    "\n",
    "\n",
    "def get_word_span(context, wordss, start, stop):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    idxs = []\n",
    "    for sent_idx, spans in enumerate(spanss):\n",
    "        for word_idx, span in enumerate(spans):\n",
    "            if not (stop <= span[0] or start >= span[1]):\n",
    "                idxs.append((sent_idx, word_idx))\n",
    "    #print(spanss)\n",
    "    #print(start,stop)\n",
    "    #print(context[start:stop])\n",
    "    assert len(idxs) > 0, \"{} {} {} {}\".format(context, spanss, start, stop)\n",
    "    return idxs[0], (idxs[-1][0], idxs[-1][1] + 1)\n",
    "def get_word_idx(context, wordss, idx):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    return spanss[idx[0]][idx[1]][0]\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens1(tokens):\n",
    "   \n",
    "    \n",
    "    tokens = [w for w in tokens if w not in set(string.punctuation)]\n",
    "   # stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens(temp_tokens):\n",
    "    tokens = []\n",
    "    \n",
    "    for token in temp_tokens:\n",
    "        flag = False\n",
    "        l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\", \"/\", \"~\", '\"', \"'\", \"\\u201C\", \"\\u2019\", \"\\u201D\", \"\\u2018\", \"\\u00B0\")\n",
    "        # \\u2013 is en-dash. Used for number to nubmer\n",
    "        # l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\")\n",
    "        # l = (\"\\u2013\",)\n",
    "        tokens.extend(re.split(\"([{}])\".format(\"\".join(l)), token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def generatedateset(sentence_list,question_list):\n",
    "    s_list=[]\n",
    "    q_list=[]\n",
    "    a_list=[]\n",
    "    res=[]\n",
    "    \n",
    "    for q in question_list:\n",
    "        if(len(q)==0):\n",
    "            continue\n",
    "        ai=q[0][0]\n",
    "      \n",
    "        pi=q[0][1]\n",
    "        si=q[0][2]\n",
    "        \n",
    "        for i in range(len(sentence_list[ai][pi])):\n",
    "            if(i==si):\n",
    "                res+=[[q[0][3],sentence_list[ai][pi][i],q[0][4], q[0][5]]]\n",
    "            else:\n",
    "                res+=[[q[0][3],sentence_list[ai][pi][i],0, q[0][5]]]\n",
    "    return res\n",
    "\n",
    "def process(data):\n",
    "    q_trn=[]\n",
    "    s_trn=[]\n",
    "    y_trn=[]\n",
    "    q_id=[]\n",
    "    for i in range(len(data)):\n",
    "       # tokens = [w for w in tokens if w not in set(string.punctuation) ]\n",
    "        \n",
    "        \n",
    "        question=list(filter(None, data[i][0]))#filter empty strin\n",
    "        sentence=list(filter(None, data[i][1]))#filter empty strin\n",
    "        index=data[i][2]\n",
    "       # list1+=[[question,sentence,index]]\n",
    "        \n",
    "        q_trn+=[question]\n",
    "        s_trn+=[sentence]\n",
    "        y_trn+=[index]\n",
    "        q_id+=[data[i][3]]\n",
    "    return q_trn,s_trn,y_trn, q_id\n",
    "\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=51):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "\n",
    "def load_data(data_type):\n",
    "    source_path = \"{}.json\".format(data_type)\n",
    "    source_data = json.load(open(source_path, 'r'))\n",
    "    context_list=[]\n",
    "    sentence_list=[]\n",
    "    question_list=[]\n",
    "    answer_list=[]\n",
    "    answer_index=[]\n",
    "    label_index=[]\n",
    "    label=[]\n",
    "    idxs=[]\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for ai, article in enumerate(tqdm(source_data['data'])):\n",
    "        s=[]\n",
    "        p = []\n",
    "        for pi, para in enumerate(article['paragraphs']):\n",
    "            # wordss\n",
    "            context = para['context']\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            si = list(map(word_tokenize, sent_tokenize(context)))\n",
    "           # print(si)\n",
    "            si = [process_tokens(tokens) for tokens in si]  # process tokens\n",
    "            for sentence in si:\n",
    "                for word in sentence:\n",
    "                     word_counter[word] += 1\n",
    "\n",
    "            # given xi, add chars\n",
    "            s.append(si)\n",
    "            p.append(context)\n",
    "\n",
    "            indexi = [ai, pi]\n",
    "            \n",
    "            for qa in para['qas']:\n",
    "                    qi = word_tokenize(qa['question'])\n",
    "                    if qa['is_impossible']:\n",
    "                        labeli=[0]\n",
    "                      #  print(labeli)\n",
    "                    else:\n",
    "                        labeli=[1]       \n",
    "                    yi = []\n",
    "                    yyi=[] \n",
    "                    answers = []\n",
    "                    q_id = qa['id']\n",
    "                    \n",
    "                    \n",
    "                    ans=[]\n",
    "                    if labeli[0] == 1:\n",
    "                        ans=qa['answers']\n",
    "                    else:\n",
    "                        ans=qa['plausible_answers']\n",
    "                    for answer in ans:\n",
    "                        answer_text=answer['text']\n",
    "                        answers.append(answer_text)\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_stop = answer_start + len(answer_text)\n",
    "                       # print(context)\n",
    "                       # print(si)\n",
    "                        yi0, yi1 = get_word_span(context, si, answer_start, answer_stop)\n",
    "                        #w0 = xi[yi0[0]][yi0[1]]            \n",
    "                        #w1 = xi[yi1[0]][yi1[1]-1]\n",
    "                        yi.append([ai,pi,yi0[0],qi,labeli[0],q_id]) \n",
    "                       # print(yi)\n",
    "                       # print(yi)\n",
    "                        yyi.append([answer_start,answer_stop])\n",
    "                    for qij in qi:\n",
    "\n",
    "                                word_counter[qij] += 1\n",
    "                    question_list.append(qi)\n",
    "                    answer_list.append(answers)\n",
    "                    answer_index.append(yi)\n",
    "                    label.append(labeli)\n",
    "                    label_index.append(indexi)\n",
    "                   # ids.append(qa['id'])\n",
    "                    idxs.append(len(idxs))\n",
    "\n",
    "        sentence_list.append(s)\n",
    "\n",
    "        context_list.append(p)\n",
    "        \n",
    "    \n",
    "    a = {'word_counter':word_counter,'sentence_list':sentence_list,'answer_index':answer_index}\n",
    "    with open(\"{}_saved.json\".format(data_type), \"w\") as fp:\n",
    "        json.dump(a , fp) \n",
    "    print(\"saved in json\")\n",
    "    return \n",
    "def load_from_json(data_type):\n",
    "    with open(\"{}_saved3.json\".format(data_type), \"r\") as fp:\n",
    "            a=json.load(fp) \n",
    "  \n",
    "    return a['word_counter'],a['context_list'],a['sentence_list'],a['answer_index']\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=301):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "\n",
    "def make_output(output):\n",
    "    if output == 0:\n",
    "        return torch.tensor([0])\n",
    "    else:\n",
    "        return torch.tensor([1])\n",
    "def create_emb_layer(weights_matrix, num_embeddings,trainable=False):\n",
    "    _, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "    \n",
    "    #emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if not trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    else:\n",
    "        emb_layer.weight.requires_grad = True\n",
    "    emb_layer.weight.data.copy_(weights_matrix)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "\n",
    "def evaluate_data(preds, correct_mapping):\n",
    "    \n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    \n",
    "    for id, pred in enumerate(correct_mapping):\n",
    "        if pred == 0 and preds[id] == 0:\n",
    "            true_negatives += 1\n",
    "        elif pred == 1 and preds[id] == 1:\n",
    "            true_positives += 1\n",
    "        elif pred == 0 and preds[id] == 1:\n",
    "            false_positives += 1\n",
    "        elif pred == 1 and preds[id] == 0:\n",
    "            false_negatives += 1\n",
    "    print( true_positives,true_negatives,false_positives,false_negatives)\n",
    "    precision = calculate_precision(true_positives, false_positives)\n",
    "    recall = calculate_recall(true_positives, false_negatives)\n",
    "    f_1 = calculate_f1(precision, recall)\n",
    "    acc=(true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
    "    return precision, recall, f_1,acc\n",
    "\n",
    "def get_acc(preds,gold_labels):\n",
    "   correct=0;\n",
    "   total=len(preds)\n",
    "   for i in range(total):\n",
    "       if preds[i]==gold_labels[i]:\n",
    "           correct+=1\n",
    "   return correct/total\n",
    "\n",
    "\n",
    "def calculate_precision(true_positives, false_positives):\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def calculate_recall(true_positives, false_negatives):\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def get_we_slist(slist, word2idx):\n",
    "    res_slist=[]\n",
    "    for sent in slist:\n",
    "        s=[]\n",
    "        for word in sent:\n",
    "            try:\n",
    "                s.append(word2idx[word.lower()])\n",
    "            except:\n",
    "                s.append(word2idx['<UNK>'])\n",
    "        res_slist.append(s)\n",
    "                \n",
    "    return res_slist\n",
    " \n",
    "def create_prediction_file(preds, q_ids, fname):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(result, outfile)\n",
    "    \n",
    "def get_prediction_dict(preds, q_ids):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    return result\n",
    "#create_embedding_matrix\n",
    "def create_embeddingmatrix(vocab):\n",
    "    \n",
    "    initW = torch.nn.init.xavier_normal_(torch.randn([len(vocab), 301]))\n",
    "    for i,v in enumerate(vocab):\n",
    "        try:\n",
    "            initW[i]=embed[v]\n",
    "        except:\n",
    "            continue\n",
    "    embededing_matrix=initW\n",
    "    return embededing_matrix\n",
    "def generatedateset2(para_list,question_list):\n",
    "   \n",
    "    res=[]\n",
    "    \n",
    "    for q in question_list:\n",
    "       \n",
    "        ai=q[0][0]\n",
    "      \n",
    "        pi=q[0][1]\n",
    "        si=q[0][2]\n",
    "        \n",
    "        res+=[[para_list[ai][pi],' '.join(q[0][3]),q[0][4],q[0][5]]]\n",
    "       \n",
    "    return res\n",
    "\n",
    "def create_index_dict(embed,word_counter):\n",
    "    word2idx={}\n",
    "    idx2word={}\n",
    "    i=1\n",
    "    #index 0 is to map padded elements\n",
    "    word2idx['<padded>']=0\n",
    "    idx2word[0]='<padded>'\n",
    "    \n",
    "    for (key,value) in (embed.items()):\n",
    "        word2idx[key]=i\n",
    "        idx2word[i]=key\n",
    "   #     vocab=np.append(vocab,[key])\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    for key,value in (word_counter.items()):\n",
    "        key=key.lower()\n",
    "    \n",
    "        try:\n",
    "            embed[key]\n",
    "           \n",
    "        except: \n",
    "            try:\n",
    "                word2idx[key]\n",
    "            except:\n",
    "                word2idx[key]=i\n",
    "                idx2word[i]=key\n",
    "        #        vocab=np.append(vocab,[key])\n",
    "                i+=1\n",
    "    \n",
    "    word2idx['<concat>']=i\n",
    "    idx2word[i]=['<concat>']\n",
    "    i+=1\n",
    "    idx2word[i]=['<unk>']\n",
    "    word2idx['<unk>']=i\n",
    "   # assert len(word2idx.keys())==len(vocab)\n",
    "    return word2idx,idx2word,set(word2idx.keys())\n",
    "\n",
    "def create_prediction_file(preds, q_ids, fname):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(result, outfile)\n",
    "    \n",
    "def get_prediction_dict(preds, q_ids):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    return result\n",
    "#create_embedding_matrix\n",
    "# def create_embeddingmatrix(vocab):\n",
    "    \n",
    "#     initW = torch.nn.init.xavier_normal_(torch.randn([len(vocab), 301]))\n",
    "#     for i,v in enumerate(vocab):\n",
    "#         try:\n",
    "#             initW[i]=embed[v]\n",
    "#         except:\n",
    "#             continue\n",
    "#     embededing_matrix=initW\n",
    "#     return embededing_matrix\n",
    "\n",
    "\n",
    "\n",
    "def create_embeddingmatrix(word2idx):\n",
    "   \n",
    "    vocab_list=list(word2idx.keys())\n",
    "    initW = np.random.randn(len(word2idx)+1 ,300)\n",
    "    \n",
    "    for i,v in enumerate(vocab_list):\n",
    "        try:\n",
    "            initW[i]=embed[v]\n",
    "        except:\n",
    "            if v=='<padded>' or v=='<unk>':\n",
    "                initW[i]=np.zeros(300)\n",
    "\n",
    "            continue\n",
    "    embededing_matrix=initW\n",
    "    return embededing_matrix\n",
    "\n",
    "def find_index(dict1,word):\n",
    "  #  print(word)\n",
    "    word=word.lower()\n",
    "    try:\n",
    "        a= dict1[word]\n",
    "    except:\n",
    "        filterd_word=re.sub(r\"[^AZa-z1-9]\",r\"\",word)\n",
    "        try:\n",
    "            a= dict1[filterd_word]\n",
    "        except:\n",
    "            a= dict1['<unk>']\n",
    "    return a\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def padded_sentences(slist,length):\n",
    "    \n",
    "    n=len(slist)\n",
    "    result=np.zeros((n,length))\n",
    "    for i in range(n):\n",
    "        for jj,j in enumerate(slist[i]):\n",
    "            if(jj>=length):\n",
    "                break\n",
    "            result[i][jj]=find_index(word2idx,j)\n",
    "           \n",
    "    return result\n",
    "\n",
    "def combine_pq(p,q):\n",
    "   assert len(p)==len(q)\n",
    "   return [p[i]+['<concat>']+q[i] for i in range(len(p))]\n",
    "def generate_data(para_list,qs):\n",
    "    #qmax:800\n",
    "    #smax:60\n",
    "    n=len(answer_index)\n",
    "#     max_p=-1\n",
    "#     max_q=-1\n",
    "    para_=[]\n",
    "    qst_=[]\n",
    "    y=[]\n",
    "    yid=[]\n",
    "    for q in (qs):\n",
    "        ai=q[0][0]\n",
    "        \n",
    "        pi=q[0][1]\n",
    "        si=q[0][2]\n",
    "        para=para_list[ai][pi]\n",
    "        qst=q[0][3]\n",
    "        \n",
    "     #   print(len(qst))\n",
    "      #  para_tokens=list(filter(None,word_tokenize(para)))\n",
    "      #  qst_tokens=list(filter(None,qst))\n",
    "       \n",
    "#         if(len(para)>max_p):\n",
    "#             max_p=len(para)\n",
    "#         if(len(qst)>max_q):\n",
    "#             max_q=len(qst)\n",
    "        para_.append(para)\n",
    "        qst_.append(qst)\n",
    "        y.append(q[0][4])\n",
    "        yid.append(q[0][5])\n",
    "  \n",
    "    return para_,qst_,np.array(y),yid\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D:/glove.6B.300d.txt]\n",
      "\tEmbedding size: 400000\n",
      "finish loading embedding file\n"
     ]
    }
   ],
   "source": [
    "embed_file=\"D:/glove.6B.300d.txt\"\n",
    "embed=read_embedding(embed_file)\n",
    "print(\"finish loading embedding file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_type='training'\n",
    "word_counter,para_list,sentence_list,answer_index=load_from_json(data_type)\n",
    "data_type='development'\n",
    "d_word_counter,d_para_list,d_sentence_list,d_answer_index=load_from_json(data_type)\n",
    "nltk.data.path.append(\"/afs/cs.stanford.edu/u/tianzhao/tz/nltk_data\")\n",
    "sent_tokenize = nltk.sent_tokenize\n",
    "\n",
    "#X=generatedateset2(para_list,answer_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrn,qtrn,ytrn,trn_id=generate_data(para_list,answer_index)\n",
    "word2idx,idx2word,vocab=create_index_dict(embed,word_counter)\n",
    "ptrn_pad=padded_sentences(ptrn,600)\n",
    "qtrn_pad=padded_sentences(qtrn,30)\n",
    "\n",
    "combined=combine_pq(ptrn,qtrn)\n",
    "combined_pad=padded_sentences(combined,700)\n",
    "\n",
    "#embed['<padded>']=np.zeros(300)\n",
    "embedding_matrix=create_embeddingmatrix(word2idx)\n",
    "\n",
    "    #preocess devset\n",
    "pdev,qdev,ydev,dev_id=generate_data(d_para_list,d_answer_index)\n",
    "#word2idx,idx2word,vocab=create_index_dict(embed,word_counter)\n",
    "pdev_pad=padded_sentences(pdev,600)\n",
    "qdev_pad=padded_sentences(qdev,30)\n",
    "\n",
    "combined_dev=combine_pq(pdev,qdev)\n",
    "combined_pad_dev=padded_sentences(combined_dev,700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_maxlen=30\n",
    "context_maxlen=600\n",
    "EMBEDDING_DIM=300\n",
    "vocab_size=len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 600, 300)     125828400   context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 30, 300)      125828400   question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 600, 64)      63936       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 30, 64)       63936       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "simple_attention_2 (SimpleAtten (None, 128)          155776      bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 128)          49536       bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           simple_attention_2[0][0]         \n",
      "                                                                 bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 251,990,241\n",
      "Trainable params: 333,441\n",
      "Non-trainable params: 251,656,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "\n",
    "question_input = Input(shape=(question_maxlen,), dtype='int32', name='question_input')\n",
    "#question_input = Input(batch_shape=(None,), dtype='float32', name='question_input')\n",
    "context_input = Input(shape=(context_maxlen,), dtype='int32', name='context_input')\n",
    "#context_input = Input(batch_shape=(None,), dtype='float32', name='context_input')\n",
    "\n",
    "questionEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
    "                         mask_zero=False, weights=[embedding_matrix], \n",
    "                         input_length=question_maxlen, trainable=False)(question_input)\n",
    "contextEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
    "                         mask_zero=False, weights=[embedding_matrix], \n",
    "                         input_length=context_maxlen, trainable=False)(context_input)\n",
    "\n",
    "Qst = Bidirectional(GRU(32, return_sequences=True))(questionEmbd)\n",
    "C = Bidirectional(GRU(32, return_sequences=True))(contextEmbd)\n",
    "#Q1 = Bidirectional(GRU(, return_sequences=True))(Q)\n",
    "#D1 = Bidirectional(GRU(96, return_sequences=True))(D)\n",
    "Qst2 = Bidirectional(GRU(64, return_sequences=False))(Qst)\n",
    "C2 = Bidirectional(GRU(64, return_sequences=False))(C)\n",
    "C2 = SimpleAttention(128, Q2, 128, return_sequences=False)(C)\n",
    "L=concatenate([C2,Qst2],axis=-1)\n",
    "\n",
    "y= Dense(1, activation='sigmoid')(L)\n",
    "\n",
    "model = Model(inputs=[context_input, question_input], outputs=[y])\n",
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "checkpoint1 = ModelCheckpoint(\"checkpoint/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_loss', verbose=1, save_best_only=False, mode='auto',save_weights_only=False,period=1)\n",
    "checkpoint3=ReduceLROnPlateau(patience=3, monitor='val_loss',min_lr=1e-7,factor=0.7,verbose=1)\n",
    "model.compile(optimizer=rms, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "checkpoint2 = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69596 samples, validate on 17400 samples\n",
      "Epoch 1/30\n",
      "69596/69596 [==============================] - 1424s 20ms/step - loss: 0.6698 - acc: 0.5826 - val_loss: 0.6530 - val_acc: 0.6104\n",
      "\n",
      "Epoch 00001: saving model to checkpoint/weights.01-0.65.hdf5\n",
      "Epoch 2/30\n",
      "69596/69596 [==============================] - 1456s 21ms/step - loss: 0.6463 - acc: 0.6161 - val_loss: 0.6415 - val_acc: 0.6224\n",
      "\n",
      "Epoch 00002: saving model to checkpoint/weights.02-0.64.hdf5\n",
      "Epoch 3/30\n",
      "69596/69596 [==============================] - 1522s 22ms/step - loss: 0.6270 - acc: 0.6350 - val_loss: 0.6327 - val_acc: 0.6264\n",
      "\n",
      "Epoch 00003: saving model to checkpoint/weights.03-0.63.hdf5\n",
      "Epoch 4/30\n",
      "69596/69596 [==============================] - 1489s 21ms/step - loss: 0.6056 - acc: 0.6577 - val_loss: 0.6195 - val_acc: 0.6393\n",
      "\n",
      "Epoch 00004: saving model to checkpoint/weights.04-0.62.hdf5\n",
      "Epoch 5/30\n",
      "69596/69596 [==============================] - 1437s 21ms/step - loss: 0.5861 - acc: 0.6730 - val_loss: 0.6077 - val_acc: 0.6527\n",
      "\n",
      "Epoch 00005: saving model to checkpoint/weights.05-0.61.hdf5\n",
      "Epoch 6/30\n",
      "69596/69596 [==============================] - 1418s 20ms/step - loss: 0.5680 - acc: 0.6909 - val_loss: 0.6144 - val_acc: 0.6487\n",
      "\n",
      "Epoch 00006: saving model to checkpoint/weights.06-0.61.hdf5\n",
      "Epoch 7/30\n",
      "69596/69596 [==============================] - 1426s 20ms/step - loss: 0.5538 - acc: 0.7028 - val_loss: 0.6106 - val_acc: 0.6539\n",
      "\n",
      "Epoch 00007: saving model to checkpoint/weights.07-0.61.hdf5\n",
      "Epoch 8/30\n",
      "69596/69596 [==============================] - 1410s 20ms/step - loss: 0.5378 - acc: 0.7138 - val_loss: 0.6124 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00008: saving model to checkpoint/weights.08-0.61.hdf5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00035000001662410796.\n",
      "Epoch 9/30\n",
      "69596/69596 [==============================] - 1410s 20ms/step - loss: 0.5183 - acc: 0.7289 - val_loss: 0.6175 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00009: saving model to checkpoint/weights.09-0.62.hdf5\n",
      "Epoch 10/30\n",
      "69596/69596 [==============================] - 1424s 20ms/step - loss: 0.5087 - acc: 0.7353 - val_loss: 0.6196 - val_acc: 0.6566\n",
      "\n",
      "Epoch 00010: saving model to checkpoint/weights.10-0.62.hdf5\n",
      "Epoch 11/30\n",
      "69596/69596 [==============================] - 1449s 21ms/step - loss: 0.4970 - acc: 0.7429 - val_loss: 0.6279 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00011: saving model to checkpoint/weights.11-0.63.hdf5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00024500001163687554.\n",
      "Epoch 12/30\n",
      "69596/69596 [==============================] - 1459s 21ms/step - loss: 0.4817 - acc: 0.7545 - val_loss: 0.6375 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00012: saving model to checkpoint/weights.12-0.64.hdf5\n",
      "Epoch 13/30\n",
      "69596/69596 [==============================] - 1439s 21ms/step - loss: 0.4806 - acc: 0.7550 - val_loss: 0.6405 - val_acc: 0.6599\n",
      "\n",
      "Epoch 00013: saving model to checkpoint/weights.13-0.64.hdf5\n",
      "Epoch 14/30\n",
      "69596/69596 [==============================] - 1440s 21ms/step - loss: 0.4802 - acc: 0.7554 - val_loss: 0.6412 - val_acc: 0.6598\n",
      "\n",
      "Epoch 00014: saving model to checkpoint/weights.14-0.64.hdf5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00017150000203400848.\n",
      "Epoch 15/30\n",
      "22528/69596 [========>.....................] - ETA: 15:02 - loss: 0.4773 - acc: 0.7550"
     ]
    }
   ],
   "source": [
    "model.fit([ptrn_pad, qtrn_pad], [ytrn], validation_data=[[pdev_pad,qdev_pad],ydev],epochs=30, batch_size=128, shuffle=True,callbacks=[checkpoint1,checkpoint2,checkpoint3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type=\"test\"\n",
    "t_word_counter,t_para_list,t_sentence_list,t_answer_index=load_from_json(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptst,qtst,tst_id=generate_data(t_para_list,t_answer_index)\n",
    "ptst_pad=padded_sentences(ptst,600)\n",
    "qtst_pad=padded_sentences(qtst,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path=\"checkpoint1/weights.01-0.65.hdf5\"\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4809 7259 1441 3891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.76944, 0.5527586206896552, 0.6433444816053512, 0.6935632183908046)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1=model.predict([ptst_pad,qtst_pad])\n",
    "pred=inference(pred1,0.5) \n",
    "evaluate_data(pred,ydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result=list(zip(tst_id,pred))\n",
    "\n",
    "import pandas as pd\n",
    "labels = ['Id','Category']\n",
    "df = pd.DataFrame.from_records(result, columns=labels)\n",
    "df.to_csv(\"lr_sample8.csv\",index=False,header=True,sep=\",\")\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
