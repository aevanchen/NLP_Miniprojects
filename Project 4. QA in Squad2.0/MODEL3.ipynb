{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc((s))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_spans(text, tokenss):\n",
    "    spanss = []\n",
    "    cur_idx = 0\n",
    "    for tokens in tokenss:\n",
    "        spans = []\n",
    "        for token in tokens:\n",
    "            if text.find(token, cur_idx) < 0:\n",
    "                print(tokens)\n",
    "                print(\"{} {} {}\".format(token, cur_idx, text))\n",
    "                raise Exception()\n",
    "            cur_idx = text.find(token, cur_idx)\n",
    "            spans.append((cur_idx, cur_idx + len(token)))\n",
    "            cur_idx += len(token)\n",
    "        spanss.append(spans)\n",
    "    return spanss\n",
    "\n",
    "\n",
    "def get_word_span(context, wordss, start, stop):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    idxs = []\n",
    "    for sent_idx, spans in enumerate(spanss):\n",
    "        for word_idx, span in enumerate(spans):\n",
    "            if not (stop <= span[0] or start >= span[1]):\n",
    "                idxs.append((sent_idx, word_idx))\n",
    "    #print(spanss)\n",
    "    #print(start,stop)\n",
    "    #print(context[start:stop])\n",
    "    assert len(idxs) > 0, \"{} {} {} {}\".format(context, spanss, start, stop)\n",
    "    return idxs[0], (idxs[-1][0], idxs[-1][1] + 1)\n",
    "def get_word_idx(context, wordss, idx):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    return spanss[idx[0]][idx[1]][0]\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens1(tokens):\n",
    "   \n",
    "    \n",
    "    tokens = [w for w in tokens if w not in set(string.punctuation)]\n",
    "   # stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens(temp_tokens):\n",
    "    tokens = []\n",
    "    \n",
    "    for token in temp_tokens:\n",
    "        flag = False\n",
    "        l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\", \"/\", \"~\", '\"', \"'\", \"\\u201C\", \"\\u2019\", \"\\u201D\", \"\\u2018\", \"\\u00B0\")\n",
    "        # \\u2013 is en-dash. Used for number to nubmer\n",
    "        # l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\")\n",
    "        # l = (\"\\u2013\",)\n",
    "        tokens.extend(re.split(\"([{}])\".format(\"\".join(l)), token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def generatedateset(sentence_list,question_list):\n",
    "    s_list=[]\n",
    "    q_list=[]\n",
    "    a_list=[]\n",
    "    res=[]\n",
    "    \n",
    "    for q in question_list:\n",
    "        if(len(q)==0):\n",
    "            continue\n",
    "        ai=q[0][0]\n",
    "      \n",
    "        pi=q[0][1]\n",
    "        si=q[0][2]\n",
    "        \n",
    "        for i in range(len(sentence_list[ai][pi])):\n",
    "            if(i==si):\n",
    "                res+=[[q[0][3],sentence_list[ai][pi][i],q[0][4], q[0][5]]]\n",
    "            else:\n",
    "                res+=[[q[0][3],sentence_list[ai][pi][i],0, q[0][5]]]\n",
    "    return res\n",
    "def create_index_dict(embed,word_counter):\n",
    "    word2idx={}\n",
    "    idx2word={}\n",
    "    i=1\n",
    "    word2idx['<UNK>']=0\n",
    "    idx2word[0]='<UNK>'\n",
    "    vocab=np.array(['<UNK>'])\n",
    "    for (key,value) in (embed.items()):\n",
    "        word2idx[key]=i\n",
    "        idx2word[i]=key\n",
    "   #     vocab=np.append(vocab,[key])\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    for key,value in (word_counter.items()):\n",
    "        key=key.lower()\n",
    "    \n",
    "        try:\n",
    "            embed[key]\n",
    "           \n",
    "        except: \n",
    "            try:\n",
    "                word2idx[key]\n",
    "            except:\n",
    "                word2idx[key]=i\n",
    "                idx2word[i]=key\n",
    "        #        vocab=np.append(vocab,[key])\n",
    "                i+=1\n",
    "   \n",
    "   # assert len(word2idx.keys())==len(vocab)\n",
    "    return word2idx,idx2word,set(word2idx.keys())\n",
    "\n",
    "def process(data):\n",
    "    q_trn=[]\n",
    "    s_trn=[]\n",
    "    y_trn=[]\n",
    "    q_id=[]\n",
    "    for i in range(len(data)):\n",
    "       # tokens = [w for w in tokens if w not in set(string.punctuation) ]\n",
    "        \n",
    "        \n",
    "        question=list(filter(None, data[i][0]))#filter empty strin\n",
    "        sentence=list(filter(None, data[i][1]))#filter empty strin\n",
    "        index=data[i][2]\n",
    "       # list1+=[[question,sentence,index]]\n",
    "        \n",
    "        q_trn+=[question]\n",
    "        s_trn+=[sentence]\n",
    "        y_trn+=[index]\n",
    "        q_id+=[data[i][3]]\n",
    "    return q_trn,s_trn,y_trn, q_id\n",
    "\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=51):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "\n",
    "def load_data(data_type):\n",
    "    source_path = \"{}.json\".format(data_type)\n",
    "    source_data = json.load(open(source_path, 'r'))\n",
    "    context_list=[]\n",
    "    sentence_list=[]\n",
    "    question_list=[]\n",
    "    answer_list=[]\n",
    "    answer_index=[]\n",
    "    label_index=[]\n",
    "    label=[]\n",
    "    idxs=[]\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for ai, article in enumerate(tqdm(source_data['data'])):\n",
    "        s=[]\n",
    "        p = []\n",
    "        for pi, para in enumerate(article['paragraphs']):\n",
    "            # wordss\n",
    "            context = para['context']\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            si = list(map(word_tokenize, sent_tokenize(context)))\n",
    "           # print(si)\n",
    "            si = [process_tokens(tokens) for tokens in si]  # process tokens\n",
    "            for sentence in si:\n",
    "                for word in sentence:\n",
    "                     word_counter[word] += 1\n",
    "\n",
    "            # given xi, add chars\n",
    "            s.append(si)\n",
    "            p.append(context)\n",
    "\n",
    "            indexi = [ai, pi]\n",
    "            \n",
    "            for qa in para['qas']:\n",
    "                    qi = word_tokenize(qa['question'])\n",
    "                    if qa['is_impossible']:\n",
    "                        labeli=[0]\n",
    "                      #  print(labeli)\n",
    "                    else:\n",
    "                        labeli=[1]       \n",
    "                    yi = []\n",
    "                    yyi=[] \n",
    "                    answers = []\n",
    "                    q_id = qa['id']\n",
    "                    \n",
    "                    \n",
    "                    ans=[]\n",
    "                    if labeli[0] == 1:\n",
    "                        ans=qa['answers']\n",
    "                    else:\n",
    "                        ans=qa['plausible_answers']\n",
    "                    for answer in ans:\n",
    "                        answer_text=answer['text']\n",
    "                        answers.append(answer_text)\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_stop = answer_start + len(answer_text)\n",
    "                       # print(context)\n",
    "                       # print(si)\n",
    "                        yi0, yi1 = get_word_span(context, si, answer_start, answer_stop)\n",
    "                        #w0 = xi[yi0[0]][yi0[1]]            \n",
    "                        #w1 = xi[yi1[0]][yi1[1]-1]\n",
    "                        yi.append([ai,pi,yi0[0],qi,labeli[0],q_id]) \n",
    "                       # print(yi)\n",
    "                       # print(yi)\n",
    "                        yyi.append([answer_start,answer_stop])\n",
    "                    for qij in qi:\n",
    "\n",
    "                                word_counter[qij] += 1\n",
    "                    question_list.append(qi)\n",
    "                    answer_list.append(answers)\n",
    "                    answer_index.append(yi)\n",
    "                    label.append(labeli)\n",
    "                    label_index.append(indexi)\n",
    "                   # ids.append(qa['id'])\n",
    "                    idxs.append(len(idxs))\n",
    "\n",
    "        sentence_list.append(s)\n",
    "\n",
    "        context_list.append(p)\n",
    "        \n",
    "    \n",
    "    a = {'word_counter':word_counter,'sentence_list':sentence_list,'answer_index':answer_index}\n",
    "    with open(\"{}_saved.json\".format(data_type), \"w\") as fp:\n",
    "        json.dump(a , fp) \n",
    "    print(\"saved in json\")\n",
    "    return \n",
    "def load_from_json(data_type):\n",
    "    with open(\"{}_saved.json\".format(data_type), \"r\") as fp:\n",
    "            a=json.load(fp) \n",
    "  \n",
    "    return a['word_counter'],a['sentence_list'],a['answer_index']\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=51):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "\n",
    "def make_output(output):\n",
    "    if output == 0:\n",
    "        return torch.tensor([0])\n",
    "    else:\n",
    "        return torch.tensor([1])\n",
    "def create_emb_layer(weights_matrix, num_embeddings,trainable=False):\n",
    "    _, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "    \n",
    "    #emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if not trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    else:\n",
    "        emb_layer.weight.requires_grad = True\n",
    "    emb_layer.weight.data.copy_(weights_matrix)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "\n",
    "def evaluate_data(preds, correct_mapping):\n",
    "    \n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    \n",
    "    for id, pred in correct_mapping.items():\n",
    "        if pred == 0 and preds[id] == 0:\n",
    "            true_negatives += 1\n",
    "        elif pred == 1 and preds[id] == 1:\n",
    "            true_positives += 1\n",
    "        elif pred == 0 and preds[id] == 1:\n",
    "            false_positives += 1\n",
    "        elif pred == 1 and preds[id] == 0:\n",
    "            false_negatives += 1\n",
    "    print( true_positives,true_negatives,false_positives,false_negatives)\n",
    "    precision = calculate_precision(true_positives, false_positives)\n",
    "    recall = calculate_recall(true_positives, false_negatives)\n",
    "    f_1 = calculate_f1(precision, recall)\n",
    "    return precision, recall, f_1\n",
    "\n",
    "def get_acc(preds,gold_labels):\n",
    "   correct=0;\n",
    "   total=len(preds)\n",
    "   for i in range(total):\n",
    "       if preds[i]==gold_labels[i]:\n",
    "           correct+=1\n",
    "   return correct/total\n",
    "\n",
    "\n",
    "def calculate_precision(true_positives, false_positives):\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def calculate_recall(true_positives, false_negatives):\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def get_we_slist(slist, word2idx):\n",
    "    res_slist=[]\n",
    "    for sent in slist:\n",
    "        s=[]\n",
    "        for word in sent:\n",
    "            try:\n",
    "                s.append(word2idx[word.lower()])\n",
    "            except:\n",
    "                s.append(word2idx['<UNK>'])\n",
    "        res_slist.append(s)\n",
    "                \n",
    "    return res_slist\n",
    " \n",
    "def create_prediction_file(preds, q_ids, fname):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(result, outfile)\n",
    "    \n",
    "def get_prediction_dict(preds, q_ids):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    return result\n",
    "#create_embedding_matrix\n",
    "def create_embeddingmatrix(vocab):\n",
    "    \n",
    "    initW = torch.nn.init.xavier_normal_(torch.randn([len(vocab), 301]))\n",
    "    for i,v in enumerate(vocab):\n",
    "        try:\n",
    "            initW[i]=embed[v]\n",
    "        except:\n",
    "            continue\n",
    "    embededing_matrix=initW\n",
    "    return embededing_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file=\"glove.6B.300d.txt\"\n",
    "embed=read_embedding(embed_file)\n",
    "print(\"finish loading embedding file\")\n",
    "\n",
    "split=True\n",
    "nltk.data.path.append(\"/afs/cs.stanford.edu/u/tianzhao/tz/nltk_data\")\n",
    "sent_tokenize = nltk.sent_tokenize\n",
    "\n",
    "data_type='training'\n",
    "word_counter,sentence_list,answer_index=load_from_json(data_type)\n",
    "data_type='development'\n",
    "d_word_counter,d_sentence_list,d_answer_index=load_from_json(data_type)\n",
    "\n",
    "data_type='training'\n",
    "X=generatedateset(sentence_list,answer_index)   \n",
    "word2idx,idx2word,vocab=create_index_dict(embed,word_counter)\n",
    "q_trn,s_trn,y_trn, q_id=process(X)\n",
    "q_trn_i=[ [word2idx[j.lower()] for j in i] for i in q_trn]\n",
    "s_trn_i=[ [word2idx[j.lower()] for j in i] for i in s_trn]\n",
    "vocab_size=len(vocab)\n",
    "print(vocab_size)\n",
    "embedding_matrix=create_embeddingmatrix(vocab)\n",
    "\n",
    "\n",
    "\n",
    "X_Tst=generatedateset(d_sentence_list,d_answer_index)   \n",
    "q_tst,s_tst,y_tst, q_id_tst=process(X_Tst)\n",
    "q_tst_i=get_we_slist(q_tst, word2idx)\n",
    "s_tst_i=get_we_slist(s_tst, word2idx)\n",
    "\n",
    "\n",
    "y_trn_d = get_prediction_dict(y_trn, q_id)\n",
    "y_tst_d = get_prediction_dict(y_tst, q_id_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class model(nn.Module):\n",
    "\n",
    "    def __init__(self,weights_matrix,vocab_size,hidden_dim=128, trainable=False):\n",
    "        super(model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size=vocab_size\n",
    "        self.trainable=trainable\n",
    "        self.embeds,self.embedding_dim = create_emb_layer(weights_matrix, self.vocab_size,trainable=self.trainable)\n",
    "        self.encoder1 = nn.LSTM( self.embedding_dim,  self.hidden_dim)\n",
    "        self.encoder2 = nn.LSTM( self.embedding_dim,  self.hidden_dim)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.out1 = nn.Linear(2*self.hidden_dim,2)\n",
    "        self.out2 = nn.Linear(1024,2048)\n",
    "        \n",
    "        self.out3 = nn.Linear(2048,2)\n",
    "        \n",
    "        self.softmax=nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "    def compute_Loss(self, pred_vec, gold_seq):\n",
    "        return self.loss(pred_vec, gold_seq)\n",
    "        \n",
    "    def forward(self,input_question, input_sen):\n",
    "        question_vectors = self.embeds(torch.tensor(input_question))\n",
    "        sen_vectors = self.embeds(torch.tensor(input_sen))\n",
    "        \n",
    "        \n",
    "        encoder_outputs1, hidden = self.encoder1(question_vectors.view(len(question_vectors),1,-1))\n",
    "        encoder_outputs2, hidden = self.encoder2(sen_vectors.view(len(sen_vectors),1,-1))\n",
    "        \n",
    "        \n",
    "        combined=torch.cat((encoder_outputs1[-1], encoder_outputs2[-1]), 1)\n",
    "        \n",
    "        prediction = self.out1(combined)\n",
    "        prediction=torch.nn.functional.relu(prediction)\n",
    "        \n",
    "        #prediction = self.out2(prediction)\n",
    "      #  prediction=torch.nn.functional.relu(prediction)\n",
    "        \n",
    "        #prediction = self.out3(prediction)\n",
    "       # prediction=torch.nn.functional.relu(prediction)\n",
    "        \n",
    "        prediction = prediction.squeeze()\n",
    "        val, idx = torch.max(prediction, 0)\n",
    "       # print(val)\n",
    "       \n",
    "        return prediction, idx.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "trainable=True\n",
    "\n",
    "m = model(weights_matrix=embedding_matrix, vocab_size=vocab_size,hidden_dim = 128,trainable = trainable)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), lr=0.01)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    m = m.cuda()\n",
    "    print ('USE GPU')\n",
    "else:\n",
    "    print ('USE CPU')\n",
    "\n",
    "\n",
    "\n",
    "minibatch_size = 2\n",
    "    \n",
    "num_minibatches = len(q_trn_i) // minibatch_size \n",
    "\n",
    "for epoch in (range(3)):\n",
    "        # Training\n",
    "        print(\"Training\")\n",
    "        # Put the model in evaluation mode\n",
    "        m.cuda()\n",
    "        m.train()\n",
    "        start_train = time.time()\n",
    "        for group in tqdm(range(num_minibatches)):\n",
    "            predictions = None\n",
    "            gold_outputs = None\n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            for i in range(group * minibatch_size, (group + 1) * minibatch_size):\n",
    "               # print(i)\n",
    "                question_vectors = q_trn_i[i]\n",
    "                sen_vectors = s_trn_i[i]\n",
    "                gold_output = make_output(y_trn[i])\n",
    "             \n",
    "                if use_gpu:\n",
    "                \n",
    "                    question_vectors  = torch.cuda.LongTensor(question_vectors)\n",
    "                    sen_vectors = torch.cuda.LongTensor(sen_vectors)\n",
    "                    gold_output = gold_output.cuda()\n",
    "                prediction_vec, prediction = m(question_vectors,sen_vectors)\n",
    "             \n",
    "                if predictions is None:\n",
    "                    predictions = [prediction_vec]\n",
    "                    gold_outputs = [gold_output] \n",
    "                else:\n",
    "                    predictions.append(prediction_vec)\n",
    "                    gold_outputs.append(gold_output)\n",
    "            #print(gold_outputs)\n",
    "            if(minibatch_size>1):\n",
    "               # print(torch.stack(predictions),torch.stack(gold_outputs).squeeze())\n",
    "                loss = m.compute_Loss(torch.stack(predictions), torch.stack(gold_outputs).squeeze())\n",
    "              \n",
    "            else:\n",
    "\n",
    "                loss = m.compute_Loss(prediction_vec.view(1,-1), gold_output)\n",
    "            \n",
    "           # print(gold_outputs)\n",
    "           # if(i%100==1):\n",
    "            #    print(loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Training time: {} for epoch {}\".format(time.time() - start_train, epoch))\n",
    "        \n",
    "        \n",
    "        print(\"Evaluation\")\n",
    "        # Put the model in evaluation mode\n",
    "     \n",
    "        m.eval()\n",
    "        start_eval = time.time()\n",
    "\n",
    "        #Inference phase\n",
    "       \n",
    "        predictions = None\n",
    "        for q_vec, s_vec  in zip(q_tst_i, s_tst_i):\n",
    "            if use_gpu:\n",
    "\n",
    "                q_vec  = torch.cuda.LongTensor(q_vec)\n",
    "                s_vec = torch.cuda.LongTensor(s_vec)\n",
    "               \n",
    "            _, predicted_output = m(q_vec,s_vec)\n",
    "            if predictions is None:\n",
    "                predictions = [predicted_output]\n",
    "                   \n",
    "            else:\n",
    "                predictions.append(predicted_output)\n",
    "   \n",
    "        #print(predictions)\n",
    "        accu=get_acc(predictions,y_tst)\n",
    "        #preds_d = get_prediction_dict(predictions, q_id)\n",
    "        #print(preds_d)\n",
    "        #print(y_te)\n",
    "       # print(preds_d)\n",
    "       # p, r, f = evaluate_data(preds_d, y_te)\n",
    "        \n",
    "        \n",
    "        #print(\"Evaluation time: {} for epoch {}, presion: {}, recall: {}, F1: {},\".format(time.time() - start_eval, epoch, p,r,f))\n",
    "        print(\"Evaluation time: {} for epoch {}, acc ï¼š{},\".format(time.time() - start_eval, epoch,accu))\n",
    "\n",
    "print(\"Done training\")\n",
    "\n",
    "PATH=\"model_1_%s.pth\"%(str(trainable))\n",
    "torch.save(m.state_dict(), PATH)\n",
    "print(\"save model completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
