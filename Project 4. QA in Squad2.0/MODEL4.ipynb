{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix #need this if you want to save tfidf_matrix\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import warnings\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc((s))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_spans(text, tokenss):\n",
    "    spanss = []\n",
    "    cur_idx = 0\n",
    "    for tokens in tokenss:\n",
    "        spans = []\n",
    "        for token in tokens:\n",
    "            if text.find(token, cur_idx) < 0:\n",
    "                print(tokens)\n",
    "                print(\"{} {} {}\".format(token, cur_idx, text))\n",
    "                raise Exception()\n",
    "            cur_idx = text.find(token, cur_idx)\n",
    "            spans.append((cur_idx, cur_idx + len(token)))\n",
    "            cur_idx += len(token)\n",
    "        spanss.append(spans)\n",
    "    return spanss\n",
    "\n",
    "\n",
    "def get_word_span(context, wordss, start, stop):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    idxs = []\n",
    "    for sent_idx, spans in enumerate(spanss):\n",
    "        for word_idx, span in enumerate(spans):\n",
    "            if not (stop <= span[0] or start >= span[1]):\n",
    "                idxs.append((sent_idx, word_idx))\n",
    "    #print(spanss)\n",
    "    #print(start,stop)\n",
    "    #print(context[start:stop])\n",
    "    assert len(idxs) > 0, \"{} {} {} {}\".format(context, spanss, start, stop)\n",
    "    return idxs[0], (idxs[-1][0], idxs[-1][1] + 1)\n",
    "def get_word_idx(context, wordss, idx):\n",
    "    spanss = get_2d_spans(context, wordss)\n",
    "    return spanss[idx[0]][idx[1]][0]\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens1(tokens):\n",
    "   \n",
    "    \n",
    "    tokens = [w for w in tokens if w not in set(string.punctuation)]\n",
    "   # stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def process_tokens(temp_tokens):\n",
    "    tokens = []\n",
    "    \n",
    "    for token in temp_tokens:\n",
    "        flag = False\n",
    "        l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\", \"/\", \"~\", '\"', \"'\", \"\\u201C\", \"\\u2019\", \"\\u201D\", \"\\u2018\", \"\\u00B0\")\n",
    "        # \\u2013 is en-dash. Used for number to nubmer\n",
    "        # l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\")\n",
    "        # l = (\"\\u2013\",)\n",
    "        tokens.extend(re.split(\"([{}])\".format(\"\".join(l)), token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def generatedateset(sentence_list,question_list):\n",
    "    s_list=[]\n",
    "    q_list=[]\n",
    "    a_list=[]\n",
    "    res=[]\n",
    "    \n",
    "    for q in question_list:\n",
    "        if(len(q)==0):\n",
    "            continue\n",
    "        ai=q[0][0]\n",
    "      \n",
    "        pi=q[0][1]\n",
    "        si=q[0][2]\n",
    "        \n",
    "        for i in range(len(sentence_list[ai][pi])):\n",
    "            if(i==si):\n",
    "                res+=[[q[0][3],sentence_list[ai][pi][i],q[0][4], q[0][5]]]\n",
    "            else:\n",
    "                res+=[[q[0][3],sentence_list[ai][pi][i],0, q[0][5]]]\n",
    "    return res\n",
    "def create_index_dict(embed,word_counter):\n",
    "    word2idx={}\n",
    "    idx2word={}\n",
    "    i=1\n",
    "    word2idx['<UNK>']=0\n",
    "    idx2word[0]='<UNK>'\n",
    "    vocab=np.array(['<UNK>'])\n",
    "    for (key,value) in (embed.items()):\n",
    "        word2idx[key]=i\n",
    "        idx2word[i]=key\n",
    "   #     vocab=np.append(vocab,[key])\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    for key,value in (word_counter.items()):\n",
    "        key=key.lower()\n",
    "    \n",
    "        try:\n",
    "            embed[key]\n",
    "           \n",
    "        except: \n",
    "            try:\n",
    "                word2idx[key]\n",
    "            except:\n",
    "                word2idx[key]=i\n",
    "                idx2word[i]=key\n",
    "        #        vocab=np.append(vocab,[key])\n",
    "                i+=1\n",
    "   \n",
    "   # assert len(word2idx.keys())==len(vocab)\n",
    "    return word2idx,idx2word,set(word2idx.keys())\n",
    "\n",
    "def process(data):\n",
    "    q_trn=[]\n",
    "    s_trn=[]\n",
    "    y_trn=[]\n",
    "    q_id=[]\n",
    "    for i in range(len(data)):\n",
    "       # tokens = [w for w in tokens if w not in set(string.punctuation) ]\n",
    "        \n",
    "        \n",
    "        question=list(filter(None, data[i][0]))#filter empty strin\n",
    "        sentence=list(filter(None, data[i][1]))#filter empty strin\n",
    "        index=data[i][2]\n",
    "       # list1+=[[question,sentence,index]]\n",
    "        \n",
    "        q_trn+=[question]\n",
    "        s_trn+=[sentence]\n",
    "        y_trn+=[index]\n",
    "        q_id+=[data[i][3]]\n",
    "    return q_trn,s_trn,y_trn, q_id\n",
    "\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=51):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "\n",
    "def load_data(data_type):\n",
    "    source_path = \"{}.json\".format(data_type)\n",
    "    source_data = json.load(open(source_path, 'r'))\n",
    "    context_list=[]\n",
    "    sentence_list=[]\n",
    "    question_list=[]\n",
    "    answer_list=[]\n",
    "    answer_index=[]\n",
    "    label_index=[]\n",
    "    label=[]\n",
    "    idxs=[]\n",
    "    word_counter = Counter()\n",
    "\n",
    "\n",
    "    for ai, article in enumerate(tqdm(source_data['data'])):\n",
    "        s=[]\n",
    "        p = []\n",
    "        for pi, para in enumerate(article['paragraphs']):\n",
    "            # wordss\n",
    "            context = para['context']\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            si = list(map(word_tokenize, sent_tokenize(context)))\n",
    "           # print(si)\n",
    "            si = [process_tokens(tokens) for tokens in si]  # process tokens\n",
    "            for sentence in si:\n",
    "                for word in sentence:\n",
    "                     word_counter[word] += 1\n",
    "\n",
    "            # given xi, add chars\n",
    "            s.append(si)\n",
    "            p.append(context)\n",
    "\n",
    "            indexi = [ai, pi]\n",
    "            \n",
    "            for qa in para['qas']:\n",
    "                    qi = word_tokenize(qa['question'])\n",
    "                    if qa['is_impossible']:\n",
    "                        labeli=[0]\n",
    "                      #  print(labeli)\n",
    "                    else:\n",
    "                        labeli=[1]       \n",
    "                    yi = []\n",
    "                    yyi=[] \n",
    "                    answers = []\n",
    "                    q_id = qa['id']\n",
    "                    \n",
    "                    \n",
    "                    ans=[]\n",
    "                    if labeli[0] == 1:\n",
    "                        ans=qa['answers']\n",
    "                    else:\n",
    "                        ans=qa['plausible_answers']\n",
    "                    for answer in ans:\n",
    "                        answer_text=answer['text']\n",
    "                        answers.append(answer_text)\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_stop = answer_start + len(answer_text)\n",
    "                       # print(context)\n",
    "                       # print(si)\n",
    "                        yi0, yi1 = get_word_span(context, si, answer_start, answer_stop)\n",
    "                        #w0 = xi[yi0[0]][yi0[1]]            \n",
    "                        #w1 = xi[yi1[0]][yi1[1]-1]\n",
    "                        yi.append([ai,pi,yi0[0],qi,labeli[0],q_id]) \n",
    "                       # print(yi)\n",
    "                       # print(yi)\n",
    "                        yyi.append([answer_start,answer_stop])\n",
    "                    for qij in qi:\n",
    "\n",
    "                                word_counter[qij] += 1\n",
    "                    question_list.append(qi)\n",
    "                    answer_list.append(answers)\n",
    "                    answer_index.append(yi)\n",
    "                    label.append(labeli)\n",
    "                    label_index.append(indexi)\n",
    "                   # ids.append(qa['id'])\n",
    "                    idxs.append(len(idxs))\n",
    "\n",
    "        sentence_list.append(s)\n",
    "\n",
    "        context_list.append(p)\n",
    "        \n",
    "    \n",
    "    a = {'word_counter':word_counter,'sentence_list':sentence_list,'answer_index':answer_index}\n",
    "    with open(\"{}_saved.json\".format(data_type), \"w\") as fp:\n",
    "        json.dump(a , fp) \n",
    "    print(\"saved in json\")\n",
    "    return \n",
    "def load_from_json(data_type):\n",
    "    with open(\"{}_saved2.json\".format(data_type), \"r\") as fp:\n",
    "            a=json.load(fp) \n",
    "  \n",
    "    return a['word_counter'],a['context_list'],a['sentence_list'],a['answer_index']\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=101):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "\n",
    "def make_output(output):\n",
    "    if output == 0:\n",
    "        return torch.tensor([0])\n",
    "    else:\n",
    "        return torch.tensor([1])\n",
    "def create_emb_layer(weights_matrix, num_embeddings,trainable=False):\n",
    "    _, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "    \n",
    "    #emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if not trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    else:\n",
    "        emb_layer.weight.requires_grad = True\n",
    "    emb_layer.weight.data.copy_(weights_matrix)\n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(pred, test_set):\n",
    "    \"\"\"\n",
    "\n",
    "    :param preds: A dictionary mapping question IDs to:\n",
    "                    -> 0 if the question does not have a plausible answer\n",
    "                    -> 1 if the question does have a plausible answer\n",
    "\n",
    "    :param test_set: A dictionary of the following form:\n",
    "\n",
    "    test_set[\"data\"] -> list of titles\n",
    "        title[\"title\"] -> title name\n",
    "        title[\"paragraphs\"] -> list of paragraphs\n",
    "            paragraph[\"context\"] -> data which might answer questions in this paragraph\n",
    "            paragraph[\"qas\"] -> list of questions\n",
    "                question[\"question\"] -> question content\n",
    "                question[\"id\"] -> question ID\n",
    "\n",
    "    :return: A 3-tuple consisting of (precision, recall, f1_score)\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    n=len(pred)\n",
    "    for i in range(n):\n",
    "        if pred[i] == 0 and test_set[i] == 0:\n",
    "            true_negatives += 1\n",
    "        elif pred[i] == 1 and test_set[i]  == 1:\n",
    "            true_positives += 1\n",
    "        elif pred[i]== 0 and test_set[i] == 1:\n",
    "            false_positives += 1\n",
    "        elif pred[i] == 1 and test_set[i] == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "    precision = calculate_precision(true_positives, false_positives)\n",
    "    recall = calculate_recall(true_positives, false_negatives)\n",
    "    f_1 = calculate_f1(precision, recall)\n",
    "    acc=(true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
    "    return precision, recall, f_1,acc\n",
    "\n",
    "\n",
    "def calculate_precision(true_positives, false_positives):\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def calculate_recall(true_positives, false_negatives):\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def get_we_slist(slist, word2idx):\n",
    "    res_slist=[]\n",
    "    for sent in slist:\n",
    "        s=[]\n",
    "        for word in sent:\n",
    "            try:\n",
    "                s.append(word2idx[word.lower()])\n",
    "            except:\n",
    "                s.append(word2idx['<UNK>'])\n",
    "        res_slist.append(s)\n",
    "                \n",
    "    return res_slist\n",
    " \n",
    "def create_prediction_file(preds, q_ids, fname):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    with open(fname, 'w') as outfile:\n",
    "        json.dump(result, outfile)\n",
    "    \n",
    "def get_prediction_dict(preds, q_ids):\n",
    "    if(len(preds) != len(q_ids)):\n",
    "        print(\"Error in pred len\")\n",
    "        return\n",
    "    result={}\n",
    "    for i in range(len(preds)):\n",
    "        result[q_ids[i]] = preds[i]\n",
    "        \n",
    "    return result\n",
    "#create_embedding_matrix\n",
    "def create_embeddingmatrix(vocab):\n",
    "    \n",
    "    initW = torch.nn.init.xavier_normal_(torch.randn([len(vocab), 300]))\n",
    "    for i,v in enumerate(vocab):\n",
    "        try:\n",
    "            initW[i]=embed[v]\n",
    "        except:\n",
    "            continue\n",
    "    embededing_matrix=initW\n",
    "    return embededing_matrix\n",
    "\n",
    "\n",
    "def generatedateset2(para_list,question_list):\n",
    "   \n",
    "    res=[]\n",
    "    \n",
    "    for q in question_list:\n",
    "        if(len(q)==0):\n",
    "            continue\n",
    "        ai=q[0]\n",
    "      \n",
    "        pi=q[1]\n",
    "        si=q[2]\n",
    "        \n",
    "        res+=[[para_list[ai][pi],' '.join(q[2]),0,q[3]]]\n",
    "       \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file=\"D:/glove.6B.100d.txt\"\n",
    "embed=read_embedding(embed_file)\n",
    "print(\"finish loading embedding file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=100\n",
    "\n",
    "data_type='training'\n",
    "word_counter,para_list,sentence_list,answer_index=load_from_json(data_type)\n",
    "data_type='development'\n",
    "d_word_counter,d_para_list,d_sentence_list,d_answer_index=load_from_json(data_type)\n",
    "\n",
    "\n",
    "X=generatedateset2(para_list,answer_index)\n",
    "nltk.data.path.append(\"/afs/cs.stanford.edu/u/tianzhao/tz/nltk_data\")\n",
    "sent_tokenize = nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mapping(x):\n",
    "    try:\n",
    "        a=embed[x]\n",
    "    except:\n",
    "        a=embed['unk']\n",
    "    \n",
    "    return a\n",
    "\n",
    "def sentence_tokenize(para):\n",
    "    \n",
    "    return sent_tokenize(para)\n",
    "\n",
    "max_sentence=30\n",
    "def get_avg_word_embedding(tokens,embed):\n",
    "    \n",
    "   \n",
    "    s=np.array(list((map(lambda x: mapping(x.lower()),tokens))))\n",
    "    s=np.mean(s,axis=0)\n",
    "    return s\n",
    "    \n",
    "    \n",
    "    \n",
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "\n",
    "def mapping2(x,feature_names):\n",
    "    try:\n",
    "        return embed[feature_names[x]]\n",
    "    except:\n",
    "        return embed['unk']\n",
    "\n",
    "def gettfidf_we(document,embed):\n",
    "    \n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    document=[document]\n",
    "    \n",
    "    tf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "    tfidf_matrix  = tf.fit_transform(document)\n",
    "\n",
    "    feature_names = tf.get_feature_names()\n",
    "    feature_index = tfidf_matrix [0,:].nonzero()[1]\n",
    "\n",
    "    s=sum(tfidf_matrix[0, x] for x in feature_index)\n",
    "\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[0, x]/s for x in feature_index])\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "    embedding=np.array(list(map(lambda x: mapping2(x,feature_names),feature_index)))\n",
    "\n",
    "    scores=[]\n",
    "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "          scores.append(s)\n",
    "    scores=np.array(scores).reshape((len(scores),1))\n",
    "    assert len(embedding)==len(scores)\n",
    "    result=np.sum(np.multiply(embedding,scores),axis=0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cosineimilarity(e,v):\n",
    "  \n",
    "    #Input:\n",
    "    #e = nxd input matrix with n row-vectors of dimensionality d (n is number of dictionary_keys)\n",
    "    #v = mxd input matrix with m row-vectors of dimensionality d (m is number of test samples)\n",
    "    # Output:\n",
    "    # Matrix D of size nxm\n",
    "    # s(i,j) is the cosinesimiarlity of embed(i,:) and test(j,:)\n",
    "    g=e.dot(v.T)\n",
    "    b=np.expand_dims(np.linalg.norm(e,axis=1),1)\n",
    "    a=np.expand_dims(np.linalg.norm(v,axis=1),1)\n",
    "    s=np.divide(g,np.multiply(b,a.T))\n",
    "    # ... until here\n",
    "    return s\n",
    "\n",
    "def findknn(D,k):\n",
    "    \"\"\"\n",
    "   # D=cos_distance matrix\n",
    "   # k = number of nearest neighbors to be found\n",
    "    \n",
    "   # Output:\n",
    "   # indices = kxm matrix, where indices(i,j) is the i^th nearest neighbor of xTe(j,:)\n",
    "   # dists = Euclidean distances to the respective nearest neighbors\n",
    "    \"\"\"\n",
    "  \n",
    " \n",
    "    m = D.shape[1]\n",
    "    ind = np.argsort(D, axis=0)\n",
    "    indices = ind[-k:]\n",
    "    r = np.array([_ for _ in range(m)], dtype=np.int)\n",
    "    r = np.array([r] * k)\n",
    "    \n",
    "    dists = D[indices, r]\n",
    "    \n",
    "    return indices, dists\n",
    "\n",
    "\n",
    "def get_avg_word_sim(d,q):\n",
    "   \n",
    "    d_em=np.array(list((map(lambda x: mapping(x.lower()),d))))\n",
    "    q_em=np.array(list((map(lambda x: mapping(x.lower()),q))))\n",
    "    D=cosineimilarity(d_em,q_em)\n",
    "    idx=findknn(D,1)[0].flatten()\n",
    "    return np.mean(d_em[idx],axis=0)\n",
    "def get_bleu(s,q):\n",
    "    return sentence_bleu(s,q,weights=(1,0,0,0))\n",
    "   \n",
    "def getsem(l,sb):\n",
    "  \n",
    "    return [sb.stem(i) for i in l]\n",
    "\n",
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def get_cos_similarity(s, q,max_length):\n",
    "    \n",
    "    sents=sentence_tokenize(s)\n",
    "    cos_sim=np.zeros((max_length))\n",
    "    q_vec=text_to_vector(q)\n",
    "    for i,sent in enumerate(sents):\n",
    "        if(i==max_length):\n",
    "            break\n",
    "        sent_vec=text_to_vector(sent)\n",
    "        cos_sim[i]=get_cosine(sent_vec,q_vec)\n",
    "        \n",
    "    return cos_sim\n",
    "        \n",
    "        \n",
    "def get_cosine(vec1, vec2):     \n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     WORD = re.compile(r'\\w+')\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "def get_ner_occur(q,para):\n",
    "        NER_index={\"NN\":0,\"VBD\":1,\"VB\":2,\"VBN\":3,\"JJ\":4}\n",
    "        NER_list=list(NER_index.keys())\n",
    "       \n",
    "       \n",
    "        features=np.zeros(5)\n",
    "      \n",
    "        p=pos_tag(q)\n",
    "   \n",
    "        \n",
    "        for i in range(len(p)):\n",
    "            tag=p[i][1]\n",
    "\n",
    "\n",
    "            word=p[i][0]\n",
    "          # print(word)\n",
    "\n",
    "            try:\n",
    "                \n",
    "                NER_index[tag]\n",
    "           #     print(1)\n",
    "                if(word not in para):\n",
    "                    features[NER_index[tag]]+=1\n",
    "            except:\n",
    "                pass\n",
    "  \n",
    "        return features\n",
    "def process(data,para_list,sentence_list,answer_index,mode='train'):\n",
    "    NER_index={\"NN\":0,\"VBD\":1,\"VB\":2,\"VBN\":3,\"JJ\":4}\n",
    "    NER_list=list(NER_index.keys())\n",
    "    m=-1\n",
    "    n=len(data)\n",
    "    \n",
    "    #vectorize\n",
    "    para_total=[]\n",
    "    for a in para_list:\n",
    "        for j in a:\n",
    "            para_total.append(j)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english',ngram_range=(1,1))\n",
    "    corpus=para_total\n",
    "    tt=vectorizer.fit_transform(corpus)\n",
    "    bow_dim=tt.toarray().shape[1]\n",
    "    max_sentence=30\n",
    "    \n",
    "    avg_para=np.zeros((n,embed_dim))\n",
    "    avg_qst=np.zeros((n,embed_dim))\n",
    "    weighted_para=np.zeros((n,embed_dim))\n",
    "    weighted_qst=np.zeros((n,embed_dim))\n",
    "    avg_word_sim=np.zeros((n,embed_dim))\n",
    "    bleu=np.zeros((n,1))\n",
    "    ner_occ=np.zeros((n,5))\n",
    "  #  bow=np.zeros((n,bow_dim))\n",
    "    \n",
    "    cos_sim=np.zeros((n,max_sentence))\n",
    "   # negation=np.zeros((n,1))\n",
    "    for i in tqdm(range(n)):\n",
    "        para=data[i][0]\n",
    "        q=data[i][1]\n",
    "        label=data[i][2]\n",
    "        qid=data[i][3]\n",
    "        #generate feature \n",
    "        #para=re.sub(r\"[^A-Za-z1-9\\-\\\"\\.\\,\\?\\!\\s]\",r\"\",para)\n",
    "        \n",
    "        para_tokens=list(filter(None,word_tokenize(para)))\n",
    "        q_tokens=list(filter(None,word_tokenize(q)))\n",
    "        avg_para[i]=get_avg_word_embedding(para_tokens,embed)\n",
    "        avg_qst[i]=get_avg_word_embedding(q_tokens,embed)\n",
    "\n",
    "        weighted_para[i]=gettfidf_we(para,embed)\n",
    "        weighted_qst[i]=gettfidf_we(q,embed)\n",
    "        \n",
    "        avg_word_sim[i]=get_avg_word_sim(para_tokens,q_tokens)\n",
    "        \n",
    "        \n",
    "        if(mode=='test'):\n",
    "              \n",
    "            ai=answer_index[i][0]\n",
    "            pi=answer_index[i][1]\n",
    "        else:\n",
    "            ai=answer_index[i][0][0]\n",
    "            pi=answer_index[i][0][1]\n",
    "        \n",
    "        sent_tokens=list(filter(None,sentence_list[ai][pi]))\n",
    "        bleu[i]=get_bleu(sent_tokens,q_tokens)\n",
    "        parsed_para= getsem(para_tokens,sb)\n",
    "        parsed_q=getsem(q_tokens,sb)\n",
    "        ner_occ[i]=get_ner_occur(parsed_q,parsed_para)\n",
    "        cos_sim[i]=get_cos_similarity(\" \".join(parsed_para),\" \".join(parsed_q),max_sentence)\n",
    "    res = np.column_stack((avg_word_sim,bleu,ner_occ,cos_sim,avg_para,avg_qst,weighted_para,weighted_qst))\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=generatedateset2(d_para_list,d_answer_index)\n",
    "feature=process(X,para_list,sentence_list,answer_index)\n",
    "\n",
    "feature_d=process(X_dev,d_para_list,d_sentence_list,d_answer_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file1=\"D:/feature/feature_trn.npy\"\n",
    "#file2=\"D:/feature/feature_dev.npy\"\n",
    "#np.save(file1, feature)\n",
    "#np.save(file2, feature_d)\n",
    "#print(\"done\")\n",
    "#feature=np.load(file1)\n",
    "#feature_d=np.load(file2)\n",
    "#print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "s=StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "s.fit(feature)\n",
    "feature_norm=s.transform(feature)\n",
    "\n",
    "y_trn=np.array([i[2] for i in X])\n",
    "y_dev=np.array([i[2] for i in X_dev])\n",
    "feature_d_norm=s.transform(feature_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5466666666666666, 0.5677450161155545, 0.5570064999707209, 0.5652298850574713)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=6, max_depth=2, random_state=2)\n",
    "rf.fit(feature_norm, y_trn)\n",
    "rf.score(feature_norm,y_trn)\n",
    "rf.score(feature_d_norm,y_dev)\n",
    "pred=rf.predict(feature_d_norm)\n",
    "print(evaluate(pred,y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6276797517098683\n",
      "0.6224137931034482\n",
      "(0.6155172413793103, 0.6241258741258742, 0.6197916666666666, 0.6224137931034482)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# poly_lr = LogisticRegression(solver = 'lbfgs')\n",
    "# poly_lr.fit(X_new ,trn_lbl)\n",
    "# print(poly_lr.score(Xtest_new,tst_lbl))\n",
    "a=0\n",
    "t=540\n",
    "lr = LogisticRegression(solver = 'lbfgs',C=1)\n",
    "lr.fit(feature_norm[:,a:t] ,y_trn)\n",
    "print(lr.score(feature_norm[:,a:t],y_trn))\n",
    "print(lr.score(feature_d_norm[:,a:t],y_dev))\n",
    "pred=lr.predict(feature_d_norm[:,a:t])\n",
    "print(evaluate(pred,y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6278564531702607\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#combine the dev and training \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_sum=np.vstack((feature,feature_d))\n",
    "feature_sum.shape\n",
    "s=StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "s.fit(feature_sum)\n",
    "feature_sum_norm=s.transform(feature_sum)\n",
    "\n",
    "lr = LogisticRegression(solver = 'lbfgs')\n",
    "lr.fit(feature_sum_norm ,y_trn_sum)\n",
    "print(lr.score(feature_sum_norm,y_trn_sum))\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test\n",
    "data_type='test'\n",
    "t_word_counter,t_para_list,t_sentence_list,t_question_index=load_from_json(data_type)\n",
    "X_te=generatedateset2(t_para_list,t_question_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11856/11856 [02:44<00:00, 71.99it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_te=process(X_te,t_para_list,t_sentence_list,t_question_index,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file3=\"D:/feature/feature_te.npy\"\n",
    "#np.save(file3, feature_te)\n",
    "#file3=\"D:/feature/feature_te.npy\"\n",
    "#feature_te=np.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#inference\n",
    "feature_te_norm=s.transform(feature_te)\n",
    "pred=lr.predict(feature_te_norm)\n",
    "Id=[i[-1] for i in X_te]\n",
    "result=list(zip(Id,pred))\n",
    "import pandas as pd\n",
    "labels = ['Id','Category']\n",
    "df = pd.DataFrame.from_records(result, columns=labels)\n",
    "df.to_csv(\"lr_fin1.csv\",index=False,header=True,sep=\",\")\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
