{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapdict(dict,sentence):\n",
    "    for i in sentence:\n",
    "        try:\n",
    "            dict[i]+=1\n",
    "        except:\n",
    "            dict[i]=1\n",
    "    return \n",
    "def getcount(word,content):\n",
    "   \n",
    "    dict={}\n",
    "    size=len(content)\n",
    "    for i in range(size):\n",
    "            mapdict(dict,content[i])        \n",
    "    return dict\n",
    "\n",
    "def addunkown(dictionary,vocab):\n",
    "    count=0\n",
    "    keylist=list(dictionary.keys())\n",
    "    for k in keylist:\n",
    "        if k in vocab:\n",
    "            continue\n",
    "        else:\n",
    "            count+=dictionary[k]\n",
    "            del dictionary[k]  \n",
    "    dictionary['UNK']=count\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def preprocess(content):\n",
    "     elements=content.strip('\\n').split(\"\\t\")\n",
    "     return elements\n",
    "\n",
    "\n",
    "def maptoken(dictionary,tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            dictionary[tokens[i]]\n",
    "        except:\n",
    "            tokens[i]=\"UNK\"\n",
    "    return tokens\n",
    "\n",
    "#smoothing of generation prob\n",
    "def addksmoothing1(dict,tag_dict,bio_list,word_list,train_word,train_bio,k):\n",
    "    V=len(word_list)\n",
    "    for word in word_list:\n",
    "        for t in bio_list:\n",
    "            dict[(word,t)]=k/(tag_dict[t]+k*V)\n",
    "    #map probabilty \n",
    "    for i in range(len(train_word)):\n",
    "        for j in range(len(train_word[i])):\n",
    "            word=train_word[i][j]\n",
    "            t=train_bio[i][j]\n",
    "            dict[(word,t)]+=1/(tag_dict[t]+k*V)\n",
    "    \n",
    "    return dict\n",
    "\n",
    "#smoothing of transiton prob\n",
    "def addksmoothing2(dict,bio_list,train_bio,k):\n",
    "#a=Counter(list(zip(*train_bio))[0])\n",
    "    b=bio_list[:]\n",
    "    bigram_dict={}\n",
    "    n_bio=len(b)\n",
    "    b.append(\"#\")\n",
    "    for t1 in b:\n",
    "        for t2 in b[:-1]:\n",
    "            dict[(t1,t2)]=k\n",
    "        #map probabilty \n",
    "    for i in range(len(train_bio)):\n",
    "        sequence=[\"#\"]+train_bio[i]\n",
    "        bigram = [tuple(sequence[i:i+2]) for i in range(len(sequence)-(1))]\n",
    "        for key in bigram:\n",
    "\n",
    "            dict[key]+=1\n",
    "            try:\n",
    "                bigram_dict[key[0]]+=1\n",
    "            except:\n",
    "                bigram_dict[key[0]]=1\n",
    "    for t1 in b:\n",
    "        for t2 in b[:-1]:\n",
    "            dict[(t1,t2)]= dict[(t1,t2)]/(k*n_bio+bigram_dict[t1]) \n",
    "    return dict\n",
    "\n",
    "#hmm veterbi \n",
    "def viterbi_hmm(line,bio_list,word_tag,tag_tag):\n",
    "  \n",
    "    N=len(bio_list) #state-graph of len \n",
    "    T=len(line) #observation len T\n",
    "    viterbi=np.zeros((N,T))\n",
    "    bp = np.zeros((N,T),dtype=int)\n",
    "    for i in range(N):\n",
    "        viterbi[i][0]=word_tag[(line[0],bio_list[i])]*tag_tag[(\"#\",bio_list[i])]\n",
    "        bp[i][0]=0\n",
    "    for t in range(1,T):\n",
    "        for s in range(N):\n",
    "            prob_to_state=np.zeros(N)\n",
    "            for j in range (N):\n",
    "                prob_to_state[j]=viterbi[j][t-1]*tag_tag[(bio_list[j],bio_list[s])]*word_tag[(line[t],bio_list[s])]\n",
    "            viterbi[s][t]=np.max(prob_to_state)\n",
    "            bp[s][t]=np.argmax(prob_to_state)\n",
    "    path = []\n",
    "    tag_index = int(viterbi.argmax(axis=0)[T-1])\n",
    "\n",
    "    for i in reversed(range(T)):     \n",
    "            path.append(bio_list[tag_index])\n",
    "            tag_index = bp[tag_index][i]\n",
    "\n",
    "    path = path[::-1]\n",
    "    return path\n",
    "\n",
    "#evalutation to compute precision ,recall and f1 score given by two inputs: predicted seq and correct seq\n",
    "def evaluate_model_span(predicted_seq, correct_seq):\n",
    "    T = len(predicted_seq)\n",
    "    \n",
    "    num_spans_pred = ['B-' in x for x in predicted_seq].count(True)\n",
    "    num_spans_ans = ['B-' in x for x in correct_seq].count(True)\n",
    "    \n",
    "    correct_pred=0\n",
    "    \n",
    "    for i in range(len(correct_seq)):\n",
    "        \n",
    "        if 'B-' in correct_seq[i]:\n",
    "            if correct_seq[i]==predicted_seq[i]:\n",
    "                flag=1;\n",
    "                tag_type = correct_seq[i][2:]\n",
    "                j=i+1\n",
    "                while( j<T and (correct_seq[j] == 'I-'+tag_type or predicted_seq[j] == 'I-'+tag_type)):\n",
    "\n",
    "                    if (correct_seq[j]!=predicted_seq[j]):\n",
    "                        flag=0;\n",
    "                    \n",
    "                    j=j+1             \n",
    "                \n",
    "                if(flag==1):\n",
    "                    correct_pred+=1;\n",
    "                               \n",
    "                i=j-1\n",
    "     \n",
    "    precision = correct_pred/num_spans_pred;\n",
    "    recall = correct_pred/num_spans_ans;\n",
    "    \n",
    "    f_score = (2*precision*recall)/(precision+recall)\n",
    "                \n",
    "    return precision, recall, f_score   \n",
    "\n",
    "#train test split \n",
    "def train_test_split(content,coef):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    n=len(content)\n",
    "    word=content[::3]\n",
    "    pos=content[1::3]\n",
    "    bio=content[2::3]\n",
    "    word=[preprocess(i)for i in word]\n",
    "    pos=[preprocess(i)for i in pos]\n",
    "    bio=[preprocess(i)for i in bio]\n",
    "    word_train, word_test, bio_train, bio_test = train_test_split(word, bio, test_size=1-coef, random_state=42)\n",
    "    word_train, word_test, pos_train, pos_test = train_test_split(word, pos, test_size=1-coef, random_state=42)\n",
    "    return  word_train,word_test,bio_train,bio_test\n",
    "\n",
    "#generate NER index \n",
    "def gen_tag_indexes(pred):\n",
    "    result = defaultdict(list)\n",
    "    T = len(pred)\n",
    "    for i in range(len(pred)):\n",
    "\n",
    "        if 'B-' in pred[i]:\n",
    "            flag=1;\n",
    "            tag_type = pred[i][2:]\n",
    "            j=i+1\n",
    "            while j<T and (pred[j] == 'I-'+tag_type) :\n",
    "                j=j+1             \n",
    "\n",
    "            j=j-1 #To get to last token\n",
    "\n",
    "\n",
    "            result[tag_type].append(str(i)+'-'+str(j))\n",
    "    return result\n",
    "\n",
    "\n",
    "#memm one feature\n",
    "def viterbi_memm(line,bio_list,dict,count_dict,k):\n",
    "    #print(line)\n",
    "    N=len(bio_list) #state-graph of len \n",
    "    T=len(line) #observation len T\n",
    "    viterbi=np.zeros((N,T))\n",
    "    bp = np.zeros((N,T),dtype=int)\n",
    "    for i in range(N):\n",
    "        viterbi[i][0]=getaddkprob((line[0],\"#\"),bio_list[i],dict,count_dict,bio_list,k)\n",
    "        bp[i][0]=0\n",
    "    for t in range(1,T):\n",
    "        for s in range(N):\n",
    "            prob_to_state=np.zeros(N)\n",
    "            for j in range (N):\n",
    "                \n",
    "                prob_to_state[j]=viterbi[j][t-1]*getaddkprob((line[t],bio_list[j]),bio_list[s],dict,count_dict,bio_list,k)\n",
    "            viterbi[s][t]=np.max(prob_to_state)\n",
    "            bp[s][t]=np.argmax(prob_to_state)\n",
    "    path = []\n",
    "    tag_index = int(viterbi.argmax(axis=0)[T-1])\n",
    "\n",
    "    for i in reversed(range(T)):     \n",
    "            path.append(bio_list[tag_index])\n",
    "            tag_index = bp[tag_index][i]\n",
    "\n",
    "    path = path[::-1]\n",
    "    return path\n",
    "#write result\n",
    "def write_result(filename,preds):\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['Type','Prediction'])\n",
    "        \n",
    "        filewriter.writerow(['ORG',' '.join(preds['ORG'])])\n",
    "        filewriter.writerow(['MISC',' '.join(preds['MISC'])])\n",
    "        filewriter.writerow(['PER',' '.join(preds['PER'])])\n",
    "        filewriter.writerow(['LOC',' '.join(preds['LOC'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:\n",
      "21537\n",
      "iter:1: 0.8436603557085485  0.6307098434484237 0.7218063566081727 with k parameters:  k :100 k1:0.8826556612078669 k2:5.390206333243154\n",
      "Vocab:\n",
      "21537\n",
      "iter:2: 0.8424214417744916  0.7819000643362642 0.8110332554776999 with k parameters:  k :100 k1:0.0514534033509737 k2:5.005990672459048\n",
      "Best performance: 0.8424214417744916  0.7819000643362642 0.8110332554776999 with k parameters:  k :100 k1:0.0514534033509737 k2:5.005990672459048\n",
      "Running time--- 25.52758765220642 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#open the file\n",
    "filename=\"train.txt\"\n",
    "f=open(filename,'r',encoding=\"utf8\")\n",
    "content=f.readlines()\n",
    "\n",
    "#train test split (0.8:0.2)\n",
    "word_train,word_test,bio_train,bio_test=train_test_split(content,0.8)\n",
    "word={}\n",
    "biotag={}\n",
    "word=getcount(word,word_train)\n",
    "biotag=getcount(biotag,bio_train)\n",
    "word_sorted=sorted(word.items(),key=lambda x: (x[1]))\n",
    "word_sorted=dict(word_sorted)\n",
    "vocab1=[i for i,x in word_sorted.items() if x>0]\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "ff=-1\n",
    "i=0\n",
    "\n",
    "#random serach for best k1 and k2\n",
    "iter=1\n",
    "while(i<=iter):\n",
    "            #set search range\n",
    "            k1=random.uniform(0,1)\n",
    "            k2=random.uniform(5, 7)    \n",
    "        \n",
    "            #create vocab where 100 words in the training set with count 1 will be assigned as unkown \n",
    "            k=100   \n",
    "            vocab2=np.array([i for i,x in word_sorted.items() if x==1])\n",
    "            t=random.sample(range(len(vocab2)), k)\n",
    "            vocab3=vocab2[t]\n",
    "            d=set(vocab1)-set(vocab3)\n",
    "            vocab=list(d)\n",
    "            print(\"Vocab:\")\n",
    "            print(len(vocab))\n",
    "            \n",
    "            \n",
    "            #update the dictionary add unkown key and map unknown key in the training set\n",
    "            word_sorted_un={}\n",
    "            word_sorted_un=addunkown(word_sorted,vocab)\n",
    "            word_train=[maptoken(word_sorted_un,i) for i in word_train]\n",
    "            #apply smoothing\n",
    "            bio_list=list(biotag.keys())\n",
    "            word_list=list(word_sorted_un.keys())\n",
    "            \n",
    "            \n",
    "            #map the unk key in the test set\n",
    "            #process the dev set\n",
    "            word_test=[maptoken(word_sorted_un,i) for i in word_test]\n",
    "            \n",
    "            #create generation prob dict and smooth with add-k smoothing. parameter :k1\n",
    "            word_tag={}\n",
    "            word_tag=addksmoothing1(word_tag,biotag,bio_list,word_list,word_train,bio_train,k1)\n",
    "            \n",
    "            #create transition prob dict  smooth with add-k smoothing. parameter :k2\n",
    "            tag_tag={}\n",
    "            tag_tag=addksmoothing2(tag_tag,bio_list,bio_train,k2)\n",
    "\n",
    "            #generate predicted sequence using HMM\n",
    "            predicted_seq=[]\n",
    "            for line in word_test:\n",
    "                predicted_seq+=viterbi_hmm(line,bio_list,word_tag,tag_tag)\n",
    "            predicted_seq\n",
    "            \n",
    "            #generate correct sequence\n",
    "            correct_seq=[]\n",
    "            for line in bio_test:\n",
    "                correct_seq+=line\n",
    "            p,r,f=evaluate_model_span(predicted_seq, correct_seq)\n",
    "            \n",
    "            \n",
    "            #find the best score\n",
    "            if(f>ff):\n",
    "                f_p=p\n",
    "                f_r=r\n",
    "                f_k=k\n",
    "                f_k1=k1\n",
    "                f_k2=k2\n",
    "                ff=f\n",
    "            i+=1\n",
    "            print(\"iter:\"+str(i)+\": \"+str(p)+\"  \"+str(r)+\" \"+str(f)+\" with k parameters:  k :\"+str(k)+\" k1:\"+str(k1)+\" k2:\"+str(k2)) \n",
    "print(\"Best performance: \"+str(f_p)+\"  \"+str(f_r)+\" \"+str(ff)+\" with k parameters:  k :\"+str(f_k)+\" k1:\"+str(f_k1)+\" k2:\"+str(f_k2))\n",
    "\n",
    "#report running time of number of iteraions\n",
    "print(\"Running time--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab\n",
      "24239\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#combine the training and validation set and generate prediction \n",
    "word_train,word_test,bio_train,bio_test=train_test_split(content,1)\n",
    "word={}\n",
    "biotag={}\n",
    "word=getcount(word,word_train)\n",
    "biotag=getcount(biotag,bio_train)\n",
    "word_sorted=sorted(word.items(),key=lambda x: (x[1]))\n",
    "word_sorted=dict(word_sorted)\n",
    "vocab1=[i for i,x in word_sorted.items() if x>0]\n",
    "\n",
    "\n",
    "#set parameters we gained from training \n",
    "k=100  #unknown word\n",
    "k1=0.03 # smoothing parameter for generation prob\n",
    "k2=6.1  #smoothing parameter for transition prob\n",
    "\n",
    "\n",
    "vocab2=np.array([i for i,x in word_sorted.items() if x==1])\n",
    "t=random.sample(range(len(vocab2)), k)\n",
    "vocab3=vocab2[t]\n",
    "d=set(vocab1)-set(vocab3)\n",
    "vocab=list(d)\n",
    "\n",
    "print(\"Vocab\")\n",
    "print(len(vocab))\n",
    "#update the dictionary\n",
    "word_sorted_un={}\n",
    "word_sorted_un=addunkown(word_sorted,vocab)\n",
    "word_train=[maptoken(word_sorted_un,i) for i in word_train]\n",
    "word_test=[maptoken(word_sorted_un,i) for i in word_test]\n",
    "#apply smoothing\n",
    "bio_list=list(biotag.keys())\n",
    "word_list=list(word_sorted_un.keys())\n",
    "\n",
    "#update the dictionary add unkown key and map unknown key in the training set\n",
    "word_sorted_un={}\n",
    "word_sorted_un=addunkown(word_sorted,vocab)\n",
    "word_train=[maptoken(word_sorted_un,i) for i in word_train]\n",
    "#apply smoothing\n",
    "bio_list=list(biotag.keys())\n",
    "word_list=list(word_sorted_un.keys())\n",
    "\n",
    "\n",
    "#create generation prob dict and smooth with add-k smoothing. parameter :k1\n",
    "word_tag={}\n",
    "word_tag=addksmoothing1(word_tag,biotag,bio_list,word_list,word_train,bio_train,k1)\n",
    "\n",
    "#create transition prob dict  smooth with add-k smoothing. parameter :k2\n",
    "tag_tag={}\n",
    "tag_tag=addksmoothing2(tag_tag,bio_list,bio_train,k2)\n",
    "\n",
    "#generate predicted sequence using HMM\n",
    "predicted_seq=[]\n",
    "for line in word_test:\n",
    "    predicted_seq+=viterbi_hmm(line,bio_list,word_tag,tag_tag)\n",
    "\n",
    "\n",
    "#test file processing\n",
    "filename=\"test.txt\"\n",
    "f=open(filename,'r',encoding=\"utf8\")\n",
    "content=f.readlines()\n",
    "n=len(content)\n",
    "word=content[::3]\n",
    "pos=content[1::3]\n",
    "index=content[2::3]\n",
    "word_test=[preprocess(i)for i in word]\n",
    "pos_test=[preprocess(i)for i in pos]\n",
    "index_test=[preprocess(i)for i in index]\n",
    "word_test=[maptoken(word_sorted_un,i) for i in word_test]\n",
    "\n",
    "#generatee prediction\n",
    "preds=gen_tag_indexes(predicted_seq)\n",
    "\n",
    "#write results to file\n",
    "write_result('hmm.csv', preds)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
