{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XINGYU CHEN XC374,  BYRAN MIN km567\n",
    "#preprocessing \n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from num2words import num2words\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def preprocess(filename):\n",
    "    f=open(filename,'r',encoding=\"utf8\")\n",
    "    content=f.readlines()\n",
    "    #strip() to delete extra space and  new line symbols\n",
    "    content = \" \".join(x.strip().strip('\\n').strip('\\t') for x in content)\n",
    "    #re-assemble words like there ' s , they 've, don't,can't into a consitent form \n",
    "    \n",
    "    \n",
    "\n",
    "    content= re.sub(r\" \\’ \",r\"'\",content)\n",
    "    content= re.sub(r\" \\'\",r\"'\",content)\n",
    "    content= re.sub(r\"\\' \",r\"'\",content)\n",
    "    content=re.sub(r\" n\\’t\",r\"n't\",content)\n",
    "    \n",
    "\n",
    "    #replace puntuation into period\n",
    "    #content=re.sub(r\"[/!/?]\",r' . ',content)\n",
    "    content=re.sub(r\"''\",r\" \",content)\n",
    "    content=re.sub(r\"“\",r\" \",content)\n",
    "    content=re.sub(r\"`\",\"\",content)\n",
    "    content=re.sub(r\"\\.\\.\\.\",r\"\",content)\n",
    "    content=re.sub(r\"[\\-\\—\\…]\",r\" \",content)\n",
    "    content=re.sub(r\"\\[\",r\"\",content)\n",
    "    content=re.sub(r\"\\]\",r\"\",content)\n",
    "\n",
    "\n",
    "    #replace puntuation into period\n",
    "    #content=re.sub(r\"[/!/?]\",r' . ',content)\n",
    "\n",
    "\n",
    "    def is_number(text):\n",
    "        try:\n",
    "            float(text)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    def conert_number2string(text):\n",
    "\n",
    "        t=text.split(\" \")\n",
    "        c=[ num2words((float)(a)) if is_number(a) else a for a in t ]\n",
    "        return \" \".join(c)\n",
    "    #replace number into word characters\n",
    "    content=conert_number2string(content)\n",
    "    \n",
    "    content=re.sub(r\"\\.\",r\" . \",content)\n",
    "    #content=re.sub(r\"\\'\",r\" ' \",content)\n",
    "    content=re.sub(r\"\\,\",r\" , \",content)\n",
    "    #add start / end token\n",
    "    #content=re.sub(r\"\\.\",r\"<s> . </s>\",content)\n",
    "        #add start / end token\n",
    "    content=re.sub(r\"\\.\",r\". </s> <s> \",content)\n",
    "    content=re.sub(r\"\\!\",r\"! </s> <s> \",content)\n",
    "    content=re.sub(r\"\\?\",r\"? </s> <s> \",content)\n",
    "    content='<s> '+ content\n",
    "    content=content[:len(content)-4]\n",
    "    content=re.sub(r\"\\.\",r\" \",content)\n",
    "    #tokenization with re by space\n",
    "    tokens = re.split(' ', content)\n",
    "    tokens= list(filter(None, tokens)) # Remove empty strings\n",
    "    return tokens\n",
    "\n",
    "#this function generates N gram for any N, 98j kj\n",
    "#returns total number of ngram\n",
    "#returns vocab for the ngram models\n",
    "#returns a word_dict for all the seen ngrams\n",
    "# returns a sorted probality \n",
    "def generateNgram(tokens,n):\n",
    "    ngram = [tuple(tokens[i:i+n]) for i in range(len(tokens)-(n-1))]\n",
    "    ngram_count=len(ngram)\n",
    "    word_dict = dict()\n",
    "    for token in ngram:\n",
    "        if token in word_dict.keys():\n",
    "            word_dict[token] += 1\n",
    "        else:\n",
    "            word_dict[token] = 1 \n",
    "    \n",
    "    word_list_sorted=sorted(word_dict.items(),key=lambda x: (-x[1]))\n",
    "    word_dict_sorted=dict(word_list_sorted)\n",
    "    return ngram_count,word_dict_sorted,word_list_sorted\n",
    "\n",
    "\n",
    "def loaddata(file):\n",
    "    f=open(file,'r',encoding=\"utf8\")\n",
    "    content=f.readlines()\n",
    "\n",
    "    content = \" \".join(x.strip().strip('\\n').strip('\\t') for x in content)\n",
    "    return content\n",
    "def tokennize(content):\n",
    "    \n",
    "    content= re.sub(r\" \\’ \",r\"'\",content)\n",
    "    content= re.sub(r\" \\'\",r\"'\",content)\n",
    "    content= re.sub(r\"\\' \",r\"'\",content)\n",
    "    content=re.sub(r\" n\\’t\",r\"n't\",content)\n",
    "    \n",
    "\n",
    "    #replace puntuation into period\n",
    "    #content=re.sub(r\"[/!/?]\",r' . ',content)\n",
    "    content=re.sub(r\"''\",r\" \",content)\n",
    "    content=re.sub(r\"“\",r\" \",content)\n",
    "    content=re.sub(r\"`\",\"\",content)\n",
    "    content=re.sub(r\"\\.\\.\\.\",r\"\",content)\n",
    "    content=re.sub(r\"[\\-\\—\\…]\",r\" \",content)\n",
    "    content=re.sub(r\"\\[\",r\"\",content)\n",
    "    content=re.sub(r\"\\]\",r\"\",content)\n",
    "\n",
    "\n",
    "    def is_number(text):\n",
    "        try:\n",
    "            float(text)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    def conert_number2string(text):\n",
    "\n",
    "        t=text.split(\" \")\n",
    "        c=[ num2words((float)(a)) if is_number(a) else a for a in t ]\n",
    "        return \" \".join(c)\n",
    "    #replace number into word characters\n",
    "    content=conert_number2string(content)\n",
    "     \n",
    "    content=re.sub(r\"\\.\",r\".\",content)\n",
    "    #content=re.sub(r\"\\'\",r\" ' \",content)\n",
    "    content=re.sub(r\"\\,\",r\" , \",content)\n",
    "    \n",
    "     #add start / end token\n",
    "    content=re.sub(r\"\\.\",r\". </s> <s> \",content)\n",
    "    content=re.sub(r\"\\!\",r\"! </s> <s> \",content)\n",
    "    content=re.sub(r\"\\?\",r\"? </s> <s> \",content)\n",
    "    content='<s> '+ content\n",
    "    content=content[:len(content)-4]\n",
    "    #tokenization with re by space\n",
    "    content=re.sub(r\"\\.\",r\" \",content)\n",
    "    tokens = re.split(' ', content)\n",
    "    tokens= list(filter(None, tokens)) # Remove empty strings\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def wordcount(tokens):\n",
    "    word_dict = dict()\n",
    "    for token in tokens:\n",
    "        if token in word_dict.keys():\n",
    "            word_dict[token] += 1\n",
    "        else:\n",
    "            word_dict[token] = 1 \n",
    "    word_list_sorted=dict(sorted(word_dict.items(),key=lambda x: (x[1])))\n",
    "\n",
    "    return word_list_sorted\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "#for obama corpus\n",
    "#add unkown token :UNK\" to the dictonary\n",
    "\n",
    "\n",
    "def addunkown(dictionary,vocab):\n",
    "    count=0\n",
    "    keylist=list(dictionary.keys())\n",
    "    for k in keylist:\n",
    "        if k in vocab:\n",
    "            continue\n",
    "        else:\n",
    "            count+=dictionary[k]\n",
    "            del dictionary[k]\n",
    "\n",
    "    for k in vocab:\n",
    "        if k not in list(dictionary.keys()):\n",
    "            dictionary[k]=0       \n",
    "    dictionary['UNK']=count\n",
    "    #start word probabilty is zero \n",
    "    dictionary['<s>']=1.0\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "#amp the unknown tokens on corpus \n",
    "def maptoken(dictionary,tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            dictionary[tokens[i]]\n",
    "        except:\n",
    "            tokens[i]=\"UNK\"\n",
    "    return tokens\n",
    "\n",
    "                                                         \n",
    "\n",
    "\n",
    "#apply smoothing\n",
    "import math\n",
    "def UnigramProbAdd1(n_vocab,vocab_dict,k):\n",
    "    \n",
    "    V = n_vocab\n",
    "    N = sum(vocab_dict.values())\n",
    "\n",
    "    prob_dict=dict()\n",
    "    for key,value in vocab_dict.items():\n",
    "        #unigram for key\n",
    "   \n",
    "\n",
    "        prob = ( value + k ) / ( N+k*V)\n",
    "        \n",
    "        prob_dict[key]=prob\n",
    "    #start word probabilty is zero for unigram\n",
    "    prob_dict['<s>']=1.0\n",
    "    prob_dict=dict(sorted(prob_dict.items(),key=lambda x: (-x[1])))\n",
    "    return prob_dict\n",
    "\n",
    "def BigramProbAdd1(n_vocab, unigram_dict,bigram_dict,k):\n",
    "    \n",
    "    V = n_vocab\n",
    "    prob_dict=dict()\n",
    "    for key,value in bigram_dict.items():\n",
    "        #unigram for key\n",
    "        prob = ( bigram_dict[key] + k ) / (unigram_dict[key[0]] + k*V)\n",
    "        \n",
    "        prob_dict[key]=prob\n",
    "    #corrent teh probabilty for </s> <s>\n",
    "    prob_dict[('</s>', '<s>')]=1.0\n",
    "    return prob_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def UnigramPerplexity(text,prob_dict,vocab_size):\n",
    "    prob=0\n",
    "    s=0\n",
    "    V=vocab_size\n",
    "    for token in text:\n",
    " \n",
    "        if(token=='<s>'):\n",
    "            s+=1\n",
    "        log_prob=math.log(prob_dict[token])\n",
    "       \n",
    "        #print(prob_dict[token])\n",
    "        prob+=(-log_prob)\n",
    "    #delete the start token\n",
    "    n=len(text)-s\n",
    "    prob=prob/n\n",
    "    \n",
    "    return math.exp(prob)\n",
    "#test the perplexity of devoplopment set\n",
    "def BigramPerplexity(text,prob_dict,unigram_dict,vocab_size,k):\n",
    "    prob=0\n",
    "    bigram = [tuple(text[i:i+2]) for i in range(len(text)-(1))]\n",
    "   # print(bigram)\n",
    "   \n",
    "    V=vocab_size\n",
    "    s=0\n",
    "\n",
    "    \n",
    "    for token in bigram:\n",
    "        try:\n",
    "           if(token==('</s>', '<s>')):\n",
    "                s+=1\n",
    "     \n",
    "           log_prob=math.log(prob_dict[token])\n",
    "            #print(prob_dict[token])\n",
    "        #if it's not in the dictionary, assign a probabily of 1/N+V   N is the count of uigram and V is to total vocalbuary\n",
    "        except:\n",
    "           \n",
    "            log_prob=math.log(k/(unigram_dict[token[0]]+V))\n",
    "            #print(1/(unigram_dict[token[0]]+V))\n",
    "    \n",
    "        prob+=-log_prob\n",
    "    n=len(text)-s-1\n",
    "    prob=prob/n\n",
    "    #print(s)\n",
    "    return math.exp(prob)\n",
    "def findacc(truth,preds): \n",
    "     acc=np.sum(truth==preds)/len(truth)\n",
    "     return acc\n",
    "def processtest(file):\n",
    "    f=open(file,'r',encoding=\"utf8\")\n",
    "    content=f.readlines()    \n",
    "    #content=[x.strip().strip('\\n').strip('\\t') for x in content]\n",
    "    content=[tokennize(x.strip()) for x in content]\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "#extract the key whose value is large than 1\n",
    "def getvocab(dict1,dict2):\n",
    "    vocab1=[i for i,x in dict1.items() if x>0]\n",
    "    vocab2=[i for i,x in dict2.items() if x>0]\n",
    "\n",
    "    vocab=list(set(vocab1)&set(vocab2))\n",
    "\n",
    "    t=list(set(vocab1)-set(vocab))\n",
    "    a=list(set(vocab2)-set(vocab))\n",
    "    print(len(t))\n",
    "   # print(t)\n",
    "   # print(len(a))\n",
    "    vocab1=[i for i in t if dict1[i]>=2]\n",
    "    print(len(vocab1))\n",
    "    vocab2=[i for i in a if dict2[i]>=2]\n",
    "    print(len(vocab2))\n",
    "    vocab=list(set(vocab)|set(vocab1)|set(vocab2))\n",
    "    \n",
    "    #print(len(vocab))\n",
    "    #print(len(vocab1))\n",
    "    #print(len(vocab2))\n",
    "    #print(vocab_size)\n",
    "    l1=len(vocab1)\n",
    "    l2=len(vocab2)\n",
    "    vocab1=[i for i in t if dict1[i]==1 ][:int(0.98*l1)]\n",
    "    vocab2=[i for i in a if dict2[i]==1 ][:int(0.98*l2)]\n",
    "    #print(len(vocab1))\n",
    "    #print(len(vocab2))\n",
    "    vocab=list(set(vocab)|set(vocab1)|set(vocab2))\n",
    "   # vocab_size=len(vocab)\n",
    "    print(len(vocab))\n",
    "    return vocab\n",
    "#merge two to be as our fixed vocab to estimate unkown words in the training corpus for two datasets\n",
    "\n",
    "import random\n",
    "\n",
    "def weighted_choice(choices):\n",
    "    #choice is set of the all the possible words given by a preceding sequence of words \n",
    "    #for bigram that is all the possible words followed by a specfic word\n",
    "   if choices==[]:\n",
    "      return [None]\n",
    "   #sum all the count in the choices set\n",
    "   total = sum(w for c, w in choices)\n",
    "   \n",
    "   #gerate a random number in the range of (0, total_count)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   # iterate over all the choices of  words\n",
    "   for c, w in choices:\n",
    "      if upto + w > r:\n",
    "         #once the cumulative prob exceeds the generated count r, we return the word\n",
    "         #print(c)\n",
    "         return c\n",
    "      upto += w\n",
    "\n",
    "#method 'naive' means to always generate the highest probabilty word (the word has the highest count given its preceding word)\n",
    "#method 'weighted' means to generate diversified sentences based on a weighted distribution of ngram counts \n",
    "#this function \n",
    "def generateNGramSentence( start,word_dict_sorted,n):\n",
    "    sentence=list(start)\n",
    "    words=start\n",
    "    word=start[-1]\n",
    "    # for unigram model \n",
    "    \n",
    "\n",
    "    #for other ngram model (n>=2)\n",
    "\n",
    "    while(1):\n",
    "       # if (method=='naive'):\n",
    "            #word=next((a[0][-1]  for a in word_dict_sorted if a[0][:n-1] ==words[-(n-1):]),'/s')     \n",
    "         #   word=next((a[-1]for a in word_dict_sorted.keys()  if a[:n-1] ==words[-(n-1):]),None)\n",
    "\n",
    "        if(n==1):\n",
    "            word=weighted_choice(list(word_dict_sorted.items()))[-1]\n",
    "            \n",
    "            #r=random.randint(0,len(word_dict_sorted.keys())-1)\n",
    "            #word=list(word_dict_sorted.keys())[r][0]\n",
    "        else:\n",
    "            choices=[(a,word_dict_sorted[a])   for a in word_dict_sorted if a[:n-1] ==words[-(n-1):]]\n",
    "            word=weighted_choice(choices)[-1]\n",
    "        if(n==1 and word=='<s>'):\n",
    "            continue\n",
    "        sentence.append(word)\n",
    "        #for unknown word we will not handled it here but will deal with it with smoothing \n",
    "        if(word==None):\n",
    "           print(\"ngram not found\")\n",
    "           return \n",
    "       \n",
    "        if(word=='</s>'):\n",
    "           break\n",
    "        words=words+tuple([word])\n",
    " \n",
    "    if(sentence[-1]!='</s>'):\n",
    "        sentence.append(\"</s>\")\n",
    "\n",
    "    return (\" \".join(sentence))\n",
    "def generateword( start,word_dict_sorted,n):\n",
    "    sentence=list(start)\n",
    "    words=start\n",
    "    word=start[-1]\n",
    "    # for unigram model \n",
    "    \n",
    "\n",
    "    #for other ngram model (n>=2)\n",
    "    while(1):\n",
    "        if(n==1):\n",
    "            word=weighted_choice(list(word_dict_sorted.items()))[-1]\n",
    "\n",
    "            #r=random.randint(0,len(word_dict_sorted.keys())-1)\n",
    "            #word=list(word_dict_sorted.keys())[r][0]\n",
    "        else:\n",
    "            choices=[(a,word_dict_sorted[a])   for a in word_dict_sorted if a[:n-1] ==words[-(n-1):]]\n",
    "            word=weighted_choice(choices)[-1]\n",
    "\n",
    "            sentence.append(word)\n",
    "            #for unknown word we will not handled it here but will deal with it with smoothing \n",
    "\n",
    "\n",
    "        if(word=='</s>'or word=='<s>'):\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return word\n",
    "\n",
    "def predict(a,b, t):\n",
    "    pred=[]\n",
    "    Id=[]\n",
    "    result=[]\n",
    "    for i,d in enumerate(test):\n",
    "        tokens=maptoken(dict1,d)\n",
    "        prediction=np.argmax([BigramPerplexity(tokens,bigram_trump,dict2,vocab_size,k),BigramPerplexity(tokens,bigram_obama,dict1,vocab_size,k)])\n",
    "        result.append((i,prediction))\n",
    "        pred.append(prediction)\n",
    "    return pred,result\n",
    "    \n",
    "\n",
    "def run(text1,text2):\n",
    "    tokens1=tokennize(text1)\n",
    "    tokens2=tokennize(text2)\n",
    "    #\n",
    "    dict1=wordcount(tokens1)\n",
    "    dict2=wordcount(tokens2)\n",
    "\n",
    "    vocab=getvocab(dict1,dict2)\n",
    "\n",
    "    dict1=addunkown(dict1,vocab)\n",
    "    dict2=addunkown(dict2,vocab)\n",
    "    tokens1=maptoken(dict1,tokens1)\n",
    "    tokens2=maptoken(dict2,tokens2)\n",
    "\n",
    "    #generate new bigram dicts\n",
    "    bigram_count1,bi_dict1,bigram_sorted_list1=generateNgram(tokens1,2)\n",
    "    bigram_count2,bi_dict2,bigram_sorted_list2=generateNgram(tokens2,2)\n",
    "    n_vocab=len(vocab)     \n",
    "    k=1\n",
    "    unigram_obama=UnigramProbAdd1(n_vocab,dict1,k) #\n",
    "    unigram_trump=UnigramProbAdd1(n_vocab,dict2,k)\n",
    "    bigram_obama=BigramProbAdd1(n_vocab,dict1,bi_dict1,k)\n",
    "    bigram_trump=BigramProbAdd1(n_vocab,dict2,bi_dict2,k)\n",
    "    \n",
    "    test_file=\"Assignment1_resources/test/test.txt\"\n",
    "    test=processtest(test_file)\n",
    "    \n",
    "    #unigram perplexity\n",
    "    pred=[]\n",
    "    Id=[]\n",
    "    for i,d in enumerate(test):\n",
    "        tokens=maptoken(dict1,d)\n",
    "        prediction=np.argmax([UnigramPerplexity(tokens,unigram_trump,vocab_size),UnigramPerplexity(tokens,unigram_obama,vocab_size)])\n",
    "        pred.append((i,prediction))\n",
    "     \n",
    "    labels = ['Id','Prediction']\n",
    "    df = pd.DataFrame.from_records(pred, columns=labels)\n",
    "    df.to_csv(\"unigram_1.csv\",index=False,header=True,sep=\",\")\n",
    "    print('done1')\n",
    "    \n",
    "    \n",
    "    #bigram perplexity\n",
    "    \n",
    "\n",
    "    Id=[]\n",
    "    pred=[]\n",
    "    for i,d in enumerate(test):\n",
    "        tokens=maptoken(dict1,d)\n",
    "        prediction=np.argmax([BigramPerplexity(tokens,bigram_trump,dict2,vocab_size,k),BigramPerplexity(tokens,bigram_obama,dict1,vocab_size,k)])\n",
    "        pred.append((i,prediction))\n",
    "      \n",
    "    labels = ['Id','Prediction']\n",
    "    df = pd.DataFrame.from_records(pred, columns=labels)\n",
    "    df.to_csv(\"bigram_1.csv\",index=False,header=True,sep=\",\")\n",
    "    print('done2')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sentence generation\n",
    "filename1=\"Assignment1_resources/train/obama.txt\"\n",
    "filename2=\"Assignment1_resources/train/trump.txt\"\n",
    "obama_tokens=preprocess(filename1)\n",
    "trump_tokens=preprocess(filename2)\n",
    "\n",
    "obama_unigram_count,obama_unigram_dict,obama_unigram_sorted_list=generateNgram(obama_tokens,1)\n",
    "trump_unigram_count,trump_unigram_dict,trump_unigram_sorted_list=generateNgram(trump_tokens,1)\n",
    "obama_bigram_count,obama_bigram_dict,obama_bigram_sorted_list=generateNgram(obama_tokens,2)\n",
    "trump_bigram_count,trump_bigram_dict,trump_bigram_sorted_list=generateNgram(trump_tokens,2)\n",
    "#build probabilty dictionary for generating sentences\n",
    "obama_unigram_prob_dict=dict([(i[0],i[1]/obama_unigram_count) for i in obama_unigram_sorted_list])\n",
    "trump_unigram_prob_dict=dict([(i[0],i[1]/trump_unigram_count) for i in trump_unigram_sorted_list])\n",
    "#build probabilty dictionary for generating sentences\n",
    "obama_bigram_prob_dict=dict([(i[0],i[1]/obama_unigram_dict[tuple([i[0][0]])])  for i in obama_bigram_sorted_list])\n",
    "trump_bigram_prob_dict=dict([(i[0],i[1]/trump_unigram_dict[tuple([i[0][0]])])  for i in trump_bigram_sorted_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=\"<s> we're going to\"\n",
    "start=tuple(start.split(\" \"))\n",
    "start2='<s>' \n",
    "start2=tuple(start2.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "seeded:\n",
      "obama:\n",
      "<s> we're going to who occasion , said of and serving demands billions and egypt and in days white rural we ; safety the effort consequences a went pacific and , absence about of acknowledge century out season an three first between control ” tears jobs even what after american the and men and pathway beliefs peace of hits can lines reason to to to still , because , they let arrayed the this but mr grounded in </s>\n",
      "trump:\n",
      "<s> we're going to and a i , out the i rancic talk think to get to i'm magnificent respect </s>\n",
      "\n",
      "\n",
      "unseeded:\n",
      "obama:\n",
      "<s> safe differences one this diploma the oil up resoundingly want a very : </s>\n",
      "trump:\n",
      "<s> disgrace not one – an if right been from the it was save they we're a done predicting i he's </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram\")\n",
    "print(\"seeded:\")\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start,obama_unigram_prob_dict,1))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start,trump_unigram_prob_dict,1))\n",
    "#testing sentence generaton without applying smoothing \n",
    "#first is the unigram model\n",
    "#unseeded sentence generation\n",
    "print()\n",
    "print()\n",
    "print(\"unseeded:\")\n",
    "#method='weighted'\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start2,obama_unigram_prob_dict,1))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start2,trump_unigram_prob_dict,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded:\n",
      "obama:\n",
      "<s> we're going to partner and , enforce rules , wild claims are gathered from the same sectarian tensions between , too </s>\n",
      "trump:\n",
      "<s> we're going to be one hundred billion dollars in real , and replaced with everybody wants to a truly signs of the people outside go around – and i want it or five people </s>\n",
      "\n",
      "\n",
      "unseeded:\n",
      "obama:\n",
      "<s> and the challenge </s>\n",
      "trump:\n",
      "<s> and a clown </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"seeded:\")\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start,obama_bigram_prob_dict,2))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start,trump_bigram_prob_dict,2))\n",
    "print()\n",
    "print()\n",
    "print(\"unseeded:\")\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start2,obama_bigram_prob_dict,2))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start2,trump_bigram_prob_dict,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded:\n",
      "obama:\n",
      "<s> we're going to be unleashed in the region that's not what i care about right now right now so that missions have the will to prevent terrorist attacks and saved according to one trillion over the last two days , i'll be blunt a lot slower than nascar </s>\n",
      "trump:\n",
      "<s> we're going to sign your life can be someplace else , chris , tonight we're closing up the arenas </s>\n",
      "\n",
      "\n",
      "unseeded:\n",
      "obama:\n",
      "<s> and we're not going to be smart and efficient and more just , and to protect ourselves and for them and what will happen if we have always loved what they did when i first arrived on this goal </s>\n",
      "trump:\n",
      "<s> but i don't want them in jail for eight years and have to put in a rainy rough day </s>\n"
     ]
    }
   ],
   "source": [
    "#try the trigram model\n",
    "obama_trigram_count,obama_trigram_dict,obama_trigram_sorted_list=generateNgram(obama_tokens,3)\n",
    "trump_trigram_count,trump_trigram_dict,trump_trigram_sorted_list=generateNgram(trump_tokens,3)\n",
    "obama_trigram_prob_dict=dict([(i[0],i[1]/obama_bigram_dict[i[0][0:2]])for i in obama_trigram_sorted_list])\n",
    "trump_trigram_prob_dict=dict([(i[0],i[1]/trump_bigram_dict[i[0][0:2]])for i in trump_trigram_sorted_list])\n",
    "\n",
    "print(\"seeded:\")\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start,obama_trigram_prob_dict,3))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start,trump_trigram_prob_dict,3))\n",
    "print()\n",
    "print()\n",
    "print(\"unseeded:\")\n",
    "#genrate a word using unigram\n",
    "word1_o=generateword(start2,obama_bigram_prob_dict,2)\n",
    "word2_t=generateword(start2,trump_bigram_prob_dict,2)\n",
    "start_o=start2+(word1_o,)\n",
    "start_t=start2+(word2_t,)\n",
    "\n",
    "\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start_o,obama_trigram_prob_dict,3))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start_t,trump_trigram_prob_dict,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded:\n",
      "obama:\n",
      "<s> we're going to move forward , and you don't necessarily see an improvement in standards of living for those asean countries </s>\n",
      "trump:\n",
      "<s> we're going to make our own products </s>\n",
      "\n",
      "\n",
      "unseeded:\n",
      "obama:\n",
      "<s> ” bad things happen , we make them better , we keep going </s>\n",
      "trump:\n",
      "<s> not in terms of three hours because who wants three hours ? </s>\n"
     ]
    }
   ],
   "source": [
    "#try the quadgram model\n",
    "obama_quadgram_count,obama_quadgram_dict,obama_quadgram_sorted_list=generateNgram(obama_tokens,4)\n",
    "trump_quadgram_count,trump_quadgram_dict,trump_quadgram_sorted_list=generateNgram(trump_tokens,4)\n",
    "obama_quadgram_prob_dict=dict([(i[0],i[1]/obama_trigram_dict[i[0][0:3]]) for i in obama_quadgram_sorted_list])\n",
    "trump_quadgram_prob_dict=dict([(i[0],i[1]/trump_trigram_dict[i[0][0:3]]) for i in trump_quadgram_sorted_list])\n",
    "\n",
    "print(\"seeded:\")\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start,obama_quadgram_prob_dict,4))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start,trump_quadgram_prob_dict,4))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"unseeded:\")\n",
    "word1_o=generateword(start2,obama_bigram_prob_dict,2)\n",
    "word2_t=generateword(start2,trump_bigram_prob_dict,2)\n",
    "start_o=start2+(word1_o,)\n",
    "start_t=start2+(word2_t,)\n",
    "word2_o=generateword(start_o,obama_trigram_prob_dict,3)\n",
    "word2_t=generateword(start_t,obama_trigram_prob_dict,3)\n",
    "\n",
    "start_o=start_o+(word2_o,)\n",
    "start_t=start_t+(word2_t,)\n",
    "print(\"obama:\")\n",
    "print(generateNGramSentence(start_o,obama_quadgram_prob_dict,4))\n",
    "print(\"trump:\")\n",
    "print(generateNGramSentence(start_t,trump_quadgram_prob_dict,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7302\n",
      "3265\n",
      "713\n",
      "12009\n"
     ]
    }
   ],
   "source": [
    "file1_trn=\"Assignment1_resources/train/obama.txt\"\n",
    "file2_trn=\"Assignment1_resources/train/trump.txt\"\n",
    "file1_dev=\"Assignment1_resources/development/obama.txt\"\n",
    "file2_dev=\"Assignment1_resources/development/trump.txt\"\n",
    "\n",
    "train1=loaddata(file1_trn)\n",
    "train2=loaddata(file2_trn)\n",
    "\n",
    "tokens1=tokennize(train1)\n",
    "tokens2=tokennize(train2)\n",
    "#\n",
    "dict1=wordcount(tokens1)\n",
    "dict2=wordcount(tokens2)\n",
    "\n",
    "vocab=getvocab(dict1,dict2)\n",
    "\n",
    "dict1=addunkown(dict1,vocab)\n",
    "dict2=addunkown(dict2,vocab)\n",
    "tokens1=maptoken(dict1,tokens1)\n",
    "tokens2=maptoken(dict2,tokens2)\n",
    "\n",
    "#generate new bigram dicts\n",
    "bigram_count1,bi_dict1,bigram_sorted_list1=generateNgram(tokens1,2)\n",
    "bigram_count2,bi_dict2,bigram_sorted_list2=generateNgram(tokens2,2)\n",
    "n_vocab=vocab_size=len(vocab)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.100000:0.975 1.0 0.95\n",
      "k=0.200000:0.975 1.0 0.95\n",
      "k=0.500000:0.97 1.0 0.94\n",
      "k=1.000000:0.965 0.99 0.94\n",
      "k=2.000000:0.94 0.96 0.92\n",
      "k=3.000000:0.9 0.89 0.91\n",
      "k=4.000000:0.82 0.76 0.88\n",
      "k=5.000000:0.71 0.61 0.81\n",
      "k=6.000000:0.6 0.44 0.76\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in [0.1,0.2,0.5,1,2,3,4,5,6]:\n",
    "    unigram_obama=UnigramProbAdd1(n_vocab,dict1,k) #\n",
    "    unigram_trump=UnigramProbAdd1(n_vocab,dict2,k)\n",
    "\n",
    "    bigram_obama=BigramProbAdd1(n_vocab,dict1,bi_dict1,k)\n",
    "    bigram_trump=BigramProbAdd1(n_vocab,dict2,bi_dict2,k)\n",
    "\n",
    "\n",
    "    pred=[]\n",
    "    result=[]\n",
    "    Id=[]\n",
    "    file1_dev=\"Assignment1_resources/development/obama.txt\"\n",
    "    file2_dev=\"Assignment1_resources/development/trump.txt\"\n",
    "    dev1=processtest(file1_dev)\n",
    "    dev2=processtest(file2_dev)\n",
    "    acc_set=[]\n",
    "    x_dev=np.concatenate((dev1,dev2),axis=0)\n",
    "    y_dev=np.concatenate((np.zeros(100),np.ones(100)),axis=0)\n",
    "    y_dev=np.array(y_dev,dtype=int)\n",
    "    \n",
    "    for i,d in enumerate(x_dev):\n",
    "        tokens=maptoken(dict1,d)\n",
    "        prediction=np.argmax([BigramPerplexity(tokens,bigram_trump,dict2,vocab_size,k),BigramPerplexity(tokens,bigram_obama,dict1,vocab_size,k)])\n",
    "        result.append((i,prediction))\n",
    "        pred.append(prediction)\n",
    "    acc=findacc(y_dev,pred)\n",
    "    acc1=findacc(y_dev[:100],pred[:100])\n",
    "    acc2=findacc(y_dev[100:200],pred[100:200])\n",
    "    print(\"k=%f:\"%(k)+str(acc)+\" \"+str(acc1)+ \" \"+str(acc2))\n",
    "    acc_set.append(acc)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram_obama\n",
      "534.2267639491849\n",
      "512.8591700210692\n",
      "Unigram_trump\n",
      "784.4244352872781\n",
      "340.77709765247715\n"
     ]
    }
   ],
   "source": [
    "#evlutae correctness of perplexity on dev sets\n",
    "k=1\n",
    "unigram_obama=UnigramProbAdd1(n_vocab,dict1,k) #\n",
    "unigram_trump=UnigramProbAdd1(n_vocab,dict2,k)\n",
    "\n",
    "bigram_obama=BigramProbAdd1(n_vocab,dict1,bi_dict1,k)\n",
    "bigram_trump=BigramProbAdd1(n_vocab,dict2,bi_dict2,k)\n",
    "\n",
    "dev_tokens1=preprocess(file1_dev)\n",
    "dev_tokens2=preprocess(file2_dev)\n",
    "dev1=maptoken(dict1,dev_tokens1)\n",
    "dev2=maptoken(dict2,dev_tokens2)\n",
    "print(\"Unigram_obama\")\n",
    "print(UnigramPerplexity(dev1,unigram_obama,n_vocab))\n",
    "print(UnigramPerplexity(dev2,unigram_obama,n_vocab))\n",
    "\n",
    "print(\"Unigram_trump\")\n",
    "print(UnigramPerplexity(dev1,unigram_trump,n_vocab))\n",
    "print(UnigramPerplexity(dev2,unigram_trump,n_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram_obama\n",
      "1395.8035033669266\n",
      "1363.0926652025676\n",
      "Bigram_trump\n",
      "2607.196215006014\n",
      "757.8420963590414\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram_obama\")\n",
    "print(BigramPerplexity(dev1,bigram_obama,dict1,n_vocab,1))\n",
    "print(BigramPerplexity(dev2,bigram_obama,dict2,n_vocab,1))\n",
    "\n",
    "print(\"Bigram_trump\")\n",
    "print(BigramPerplexity(dev1,bigram_trump,dict1,n_vocab,1))\n",
    "print(BigramPerplexity(dev2,bigram_trump,dict2,n_vocab,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with k being 1\n",
      "bigtram k=1.000000:0.965 0.99 0.94\n",
      "unigram: k=1.000000:0.94 0.95 0.93\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy with k being 1\")\n",
    "for k in [1]:\n",
    "    unigram_obama=UnigramProbAdd1(n_vocab,dict1,k) #\n",
    "    unigram_trump=UnigramProbAdd1(n_vocab,dict2,k)\n",
    "\n",
    "    bigram_obama=BigramProbAdd1(n_vocab,dict1,bi_dict1,k)\n",
    "    bigram_trump=BigramProbAdd1(n_vocab,dict2,bi_dict2,k)\n",
    "\n",
    "\n",
    "    pred=[]\n",
    "    result=[]\n",
    "    Id=[]\n",
    "    file1_dev=\"Assignment1_resources/development/obama.txt\"\n",
    "    file2_dev=\"Assignment1_resources/development/trump.txt\"\n",
    "    dev1=processtest(file1_dev)\n",
    "    dev2=processtest(file2_dev)\n",
    "    x_dev=np.concatenate((dev1,dev2),axis=0)\n",
    "    y_dev=np.concatenate((np.zeros(100),np.ones(100)),axis=0)\n",
    "    y_dev=np.array(y_dev,dtype=int)\n",
    "    \n",
    "    for i,d in enumerate(x_dev):\n",
    "        tokens=maptoken(dict1,d)\n",
    "        t=BigramPerplexity(tokens,bigram_trump,dict2,vocab_size,k)\n",
    "        o=BigramPerplexity(tokens,bigram_obama,dict1,vocab_size,k)\n",
    "        prediction=np.argmax([t,o])\n",
    "        result.append((i,prediction))\n",
    "        pred.append(prediction)\n",
    " \n",
    "    acc=findacc(y_dev,pred)\n",
    "    acc1=findacc(y_dev[:100],pred[:100])\n",
    "    acc2=findacc(y_dev[100:200],pred[100:200])\n",
    "    print(\"bigtram k=%f:\"%(k)+str(acc)+\" \"+str(acc1)+ \" \"+str(acc2))\n",
    "    \n",
    "    pred=[]\n",
    "\n",
    "    for i,d in enumerate(x_dev):\n",
    "        tokens=maptoken(dict1,d)\n",
    "        t=UnigramPerplexity(tokens,unigram_trump,vocab_size)\n",
    "        o=UnigramPerplexity(tokens,unigram_obama,vocab_size)\n",
    "        prediction=np.argmax([t,o])\n",
    "        result.append((i,prediction))\n",
    "        pred.append(prediction)\n",
    " \n",
    "    acc=findacc(y_dev,pred)\n",
    "    acc1=findacc(y_dev[:100],pred[:100])\n",
    "    acc2=findacc(y_dev[100:200],pred[100:200])\n",
    "    \n",
    "    print(\"unigram: k=%f:\"%(k)+str(acc)+\" \"+str(acc1)+ \" \"+str(acc2))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7207\n",
      "3289\n",
      "685\n",
      "12003\n",
      "done1\n",
      "done2\n"
     ]
    }
   ],
   "source": [
    "#now we know the best k=1\n",
    "#combine the train and dev set\n",
    "#run the prediction model\n",
    "\n",
    "file1_trn=\"Assignment1_resources/train/obama.txt\"\n",
    "file2_trn=\"Assignment1_resources/train/trump.txt\"\n",
    "file1_dev=\"Assignment1_resources/development/obama.txt\"\n",
    "file2_dev=\"Assignment1_resources/development/trump.txt\"\n",
    "\n",
    "train1=loaddata(file1_trn)+\" \"+loaddata(file1_dev)\n",
    "train2=loaddata(file2_trn)+\" \"+loaddata(file2_dev)\n",
    "\n",
    "run(train1,train2)\n",
    "print(\"predictions are recorded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2： analogy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "def read_embedding(filename):\n",
    "    embed = {}\n",
    "    for line in open(filename,encoding='utf8'):\n",
    "        line = line.strip().split()\n",
    "        if (len(line)!=301):\n",
    "            continue\n",
    "        try:\n",
    "            embed[(line[0])] = np.array(list(map(float, line[1:])))\n",
    "        except:\n",
    "            pass\n",
    "    print('[%s]\\n\\tEmbedding size: %d' % (filename, len(embed)), end='\\n')\n",
    "    return embed\n",
    "def preprocessfile(filename):\n",
    "    train_set=[]\n",
    "    train_label=[]\n",
    "    f=open(filename,'r',encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        line=re.sub('[\\t\\n]', '',line)\n",
    "        set=line.split(\" \")\n",
    "\n",
    "        train_set.append(set[0:3])\n",
    "        train_label.append(set[-1])\n",
    "    return np.array(train_set),np.array(train_label)\n",
    "\n",
    "\n",
    "def mapembedding(train_file,embeded_dict):\n",
    "    oov=[]\n",
    "    result=[]\n",
    "    a=1\n",
    "    for i in range (len(train_file)):\n",
    "          a=1\n",
    "          c=[]\n",
    "          for j in train_file[i]:\n",
    "           \n",
    "                try:\n",
    "                    e=embed[j]\n",
    "\n",
    "                except:\n",
    "                    try:\n",
    "                        e=embed[j.lower()]\n",
    "                        \n",
    "                    except:\n",
    "                        oov.append(i)\n",
    "                        a=0\n",
    "                        continue\n",
    "             \n",
    "                c.append(e)\n",
    "          if(a!=0):\n",
    "                result.append(c)\n",
    "        \n",
    "    result =np.array(result)\n",
    "        \n",
    "    return result \n",
    "def processembedding(embed):\n",
    "    result=[]\n",
    "    for i in range(len(embed)):\n",
    "        a=0\n",
    "        a=-embed[i][0]+embed[i][1]+embed[i][2]\n",
    "        result.append(a)\n",
    "    return np.array(result)\n",
    "\n",
    "def createvocab(embed_dict):\n",
    "    vocab=np.array(list(embed_dict.keys()))\n",
    "    vocab_dict={}\n",
    "    for i,w in enumerate(vocab):\n",
    "        vocab_dict[w]=i\n",
    "    return vocab,vocab_dict\n",
    "\n",
    "def createvocab_dict(vocab):\n",
    "    vocab_dict={}\n",
    "    for i,w in enumerate(vocab):\n",
    "        vocab_dict[w]=i\n",
    "    return vocab_dict\n",
    "\n",
    "def createnumpyembedding(embed_dict):\n",
    "    return np.array(list(embed_dict.values()))\n",
    "#print(x_train.shape)\n",
    "\n",
    "def findacc(truth,preds):\n",
    "     \n",
    "     acc=np.sum(truth==preds)/len(truth)\n",
    "     return acc\n",
    "def cosineimilarity(e,v):\n",
    "  \n",
    "    #Input:\n",
    "    #e = nxd input matrix with n row-vectors of dimensionality d (n is number of dictionary_keys)\n",
    "    #v = mxd input matrix with m row-vectors of dimensionality d (m is number of test samples)\n",
    "    # Output:\n",
    "    # Matrix D of size nxm\n",
    "    # s(i,j) is the cosinesimiarlity of embed(i,:) and test(j,:)\n",
    "    g=e.dot(v.T)\n",
    "    b=np.expand_dims(np.linalg.norm(e,axis=1),1)\n",
    "    a=np.expand_dims(np.linalg.norm(v,axis=1),1)\n",
    "    s=np.divide(g,np.multiply(b,a.T))\n",
    "    # ... until here\n",
    "    return s\n",
    "\n",
    "\n",
    "def findknn(D,k):\n",
    "    \"\"\"\n",
    "   # D=cos_distance matrix\n",
    "   # k = number of nearest neighbors to be found\n",
    "    \n",
    "   # Output:\n",
    "   # indices = kxm matrix, where indices(i,j) is the i^th nearest neighbor of xTe(j,:)\n",
    "   # dists = Euclidean distances to the respective nearest neighbors\n",
    "    \"\"\"\n",
    "\n",
    "    m = D.shape[1]\n",
    "    ind = np.argsort(D, axis=0)\n",
    "    indices = ind[-k:]\n",
    "    r = np.array([_ for _ in range(m)], dtype=np.int)\n",
    "    r = np.array([r] * k)\n",
    "    \n",
    "    dists = D[indices, r]\n",
    "\n",
    "    return indices, dists\n",
    "def vocab_third_col(x):\n",
    "    third_col=x[:,-1]\n",
    "    third_col_y=[]\n",
    "    for e in third_col:\n",
    "        try:\n",
    "            t=vocab_dict[e]\n",
    "        except:\n",
    "            try:\n",
    "                t=vocab_dict[e.lower()]\n",
    "            except:\n",
    "                continue\n",
    "        third_col_y.append(t)\n",
    "    third_col_y=np.array(third_col_y)\n",
    "    return third_col_y  \n",
    "\n",
    "#a is the result of nn classifer, b is the third_colmn vocab_index\n",
    "def modify(a,b):\n",
    "    d=np.ones((1,a.shape[1]),dtype=int)\n",
    "    for i in range (a.shape[1]):\n",
    "        if b[i]!=a[-1][i]:\n",
    "            d[0][i]=a[-1][i]\n",
    "        else:\n",
    "            d[0][i]=a[-2][i]\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:659\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x,y=preprocessfile(filename)\n",
    "vocab=set(x.flatten())|set(y)\n",
    "print(\"vocab_size:\"+str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D:/glove.6B.300d.txt]\n",
      "\tEmbedding size: 400000\n"
     ]
    }
   ],
   "source": [
    "#load this file to create vocab\n",
    "#embed_file=\"D:/glove.6B.300d.txt\"\n",
    "#embed=read_embedding(embed_file)\n",
    "#vocab,vocab_dict=createvocab(embed)\n",
    "#vocab=set(vocab)|set(x.flatten())|set(y)\n",
    "#print(\"vocab_size:\"+str(len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:400379\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D:/word_embedding/glove.840B.300d.txt]\n",
      "\tEmbedding size: 2195875\n",
      "finish loading embedding file\n"
     ]
    }
   ],
   "source": [
    "#the load the embedding file which is 840B the large dataset\n",
    "embed_file=\"D:/word_embedding/glove.840B.300d.txt\"\n",
    "embed=read_embedding(embed_file)\n",
    "print(\"finish loading embedding file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#create the 840B vesion of embedding based on the customized vocab\n",
    "customized_dict={}\n",
    "for v in vocab:\n",
    "    try:\n",
    "        customized_dict[v]=embed[v]\n",
    "    except:\n",
    "        #aslo we should consider decapitalized vocab\n",
    "        try: \n",
    "            customized_dict[v.title()]=embed[v.title()]\n",
    "        except:     \n",
    "            pass\n",
    "\n",
    "vocab,vocab_dict=createvocab(customized_dict)\n",
    "\n",
    "embeddings=createnumpyembedding(customized_dict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train=processembedding(mapembedding(x,customized_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for convinientence of testing we save the vocab and embedding file locally\n",
    "#np.save(\"840B_glove_vocab.npy\",vocab)\n",
    "#np.save(\"840B_glove_embeddings.npy\",embeddings)\n",
    "\n",
    "#uncomment below to load them \n",
    "#vocab=np.load(\"840B_glove_vocab.npy\")\n",
    "#embeddings=np.load(\"840B_glove_embeddings.npy\")\n",
    "#vocab_dict=createvocab_dict(vocab)\n",
    "#customized_dict={}\n",
    "#for i,v in enumerate(vocab):\n",
    " #       customized_dict[v]=embeddings[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting searching the most similar word for test file\n",
      "running the chunk 0...\n",
      "running the chunk 1...\n",
      "running the chunk 2...\n",
      "running the chunk 3...\n",
      "running the chunk 4...\n",
      "running the chunk 5...\n",
      "running the chunk 6...\n",
      "running the chunk 7...\n",
      "running the chunk 8...\n",
      "running the chunk 9...\n",
      "running the chunk 10...\n",
      "running the chunk 11...\n",
      "running the chunk 12...\n",
      "running the chunk 13...\n",
      "running the chunk 14...\n",
      "running the chunk 15...\n",
      "running the chunk 16...\n",
      "running the chunk 17...\n",
      "running the chunk 18...\n",
      "running the chunk 19...\n",
      "running the chunk 20...\n",
      "running the chunk 21...\n",
      "running the chunk 22...\n",
      "running the chunk 23...\n",
      "running the chunk 24...\n",
      "running the chunk 25...\n",
      "running the chunk 26...\n",
      "running the chunk 27...\n",
      "running the chunk 28...\n",
      "running the chunk 29...\n",
      "running the chunk 30...\n",
      "running the chunk 31...\n",
      "running the chunk 32...\n",
      "running the chunk 33...\n",
      "running the chunk 34...\n",
      "running the chunk 35...\n",
      "running the chunk 36...\n",
      "running the chunk 37...\n",
      "running the chunk 38...\n",
      "running the chunk 39...\n",
      "running the chunk 40...\n",
      "running the chunk 41...\n",
      "running the chunk 42...\n",
      "running the chunk 43...\n",
      "running the chunk 44...\n",
      "running the chunk 45...\n",
      "running the chunk 46...\n",
      "running the chunk 47...\n",
      "running the chunk 48...\n",
      "running the chunk 49...\n",
      "running the chunk 50...\n",
      "running the chunk 51...\n",
      "running the chunk 52...\n",
      "running the chunk 53...\n",
      "running the chunk 54...\n",
      "running the chunk 55...\n",
      "running the chunk 56...\n",
      "running the chunk 57...\n",
      "running the chunk 58...\n",
      "running the chunk 59...\n",
      "running the chunk 60...\n",
      "running the chunk 61...\n",
      "running the chunk 62...\n",
      "running the chunk 63...\n",
      "running the chunk 64...\n",
      "running the chunk 65...\n",
      "running the chunk 66...\n",
      "running the chunk 67...\n",
      "running the chunk 68...\n",
      "complete\n",
      "running time:0.7977027039814857\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "#go over the entrie text file\n",
    "#break file into submatrixes\n",
    "#each chunk contains 100 samples of data\n",
    "#evaluate accuracy\n",
    "truth=[vocab_dict[w] for w in y]\n",
    "number=200\n",
    "n_chunk=(math.ceil(len(x_train)/number))\n",
    "e=embeddings\n",
    "preds1=np.array([[]])\n",
    "preds2=np.array([[]])\n",
    "third_col=vocab_third_col(x)\n",
    "print(\"starting searching the most similar word for test file\")\n",
    "for i in range (n_chunk):\n",
    "    print(\"running the chunk %d...\"%(i))\n",
    "    v=x_train[number*(i):number*(i+1)]\n",
    "    b=third_col[number*(i):number*(i+1)]\n",
    "    s=cosineimilarity(e,v)\n",
    "    pred,dist= findknn(s,2)\n",
    "    preds1=np.concatenate((preds1,np.expand_dims(pred[-1],0)),axis=1)\n",
    "    preds2=np.concatenate((preds2,modify(pred,b)),axis=1)\n",
    "truth=np.array(truth).astype(int)\n",
    "preds1=np.array(preds1).astype(int).flatten()\n",
    "preds2=np.array(preds2).astype(int).flatten()\n",
    "print(\"complete\")\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"running time:\"+str(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time:0.7977027039814857 s\n"
     ]
    }
   ],
   "source": [
    "print(\"running time:\"+str(elapsed)+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 correct examples:\n",
      "data:['Harare' 'Zimbabwe' 'Niamey']          label:Niger               pred:Niger               result: correct\n",
      "\n",
      "data:['Belgrade' 'Serbia' 'Kathmandu']       label:Nepal               pred:Nepal               result: correct\n",
      "\n",
      "data:['USA' 'dollar' 'India']                label:rupee               pred:rupee               result: correct\n",
      "\n",
      "data:['son' 'daughter' 'he']                 label:she                 pred:she                 result: correct\n",
      "\n",
      "data:['Jakarta' 'Indonesia' 'Kathmandu']     label:Nepal               pred:Nepal               result: correct\n",
      "\n",
      "data:['Colombia' 'Colombian' 'Denmark']      label:Danish              pred:Danish              result: correct\n",
      "\n",
      "data:['simple' 'simpler' 'hard']             label:harder              pred:harder              result: correct\n",
      "\n",
      "data:['Sofia' 'Bulgaria' 'Zagreb']           label:Croatia             pred:Croatia             result: correct\n",
      "\n",
      "data:['Glendale' 'California' 'Louisville']  label:Kentucky            pred:Kentucky            result: correct\n",
      "\n",
      "data:['Tegucigalpa' 'Honduras' 'Tirana']     label:Albania             pred:Albania             result: correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct=truth[truth==preds1]\n",
    "correct_i=np.argwhere(truth==preds1).flatten()\n",
    "false=preds1[truth!=preds1]\n",
    "false_i=np.where([truth!=preds1])[1]\n",
    "def randomdisplay(x,y,vocab,vocab_index,data_index,n):\n",
    "    n_samples_for_display=n\n",
    "    a=np.random.choice(len(correct),n_samples_for_display,replace=False)\n",
    "    if(y[data_index[a[0]]]==vocab[vocab_index[a[0]]]):\n",
    "        status=\"correct\"\n",
    "    else:\n",
    "        status=\"incorrect\"\n",
    "    for i in range (n_samples_for_display):\n",
    "        print(\"data:{:40s}\".format((str(x[data_index[a[i]]]))) +\"label:{:20s}\".format((y[data_index[a[i]]]))\n",
    "              +\"pred:{:20s}\".format(vocab[vocab_index[a[i]]])+\"result: %s\"%(status))\n",
    "        print()\n",
    "        \n",
    "print(\"10 correct examples:\")\n",
    "randomdisplay(x,y,vocab,correct,correct_i,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 false examples:\n",
      "data:['mouse' 'mice' 'onion']                label:onions              pred:onion               result: incorrect\n",
      "\n",
      "data:['Houston' 'Texas' 'Tulsa']             label:Oklahoma            pred:Texas               result: incorrect\n",
      "\n",
      "data:['boy' 'girl' 'groom']                  label:bride               pred:groom               result: incorrect\n",
      "\n",
      "data:['he' 'she' 'groom']                    label:bride               pred:groom               result: incorrect\n",
      "\n",
      "data:['Dallas' 'Texas' 'Lexington']          label:Kentucky            pred:Lexington           result: incorrect\n",
      "\n",
      "data:['Bakersfield' 'California' 'Irving']   label:Texas               pred:Irving              result: incorrect\n",
      "\n",
      "data:['groom' 'bride' 'grandpa']             label:grandma             pred:grandpa             result: incorrect\n",
      "\n",
      "data:['eagle' 'eagles' 'building']           label:buildings           pred:building            result: incorrect\n",
      "\n",
      "data:['computer' 'computers' 'elephant']     label:elephants           pred:elephant            result: incorrect\n",
      "\n",
      "data:['Huntsville' 'Alabama' 'Oakland']      label:California          pred:Oakland             result: incorrect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"10 false examples:\")\n",
    "randomdisplay(x,y,vocab,false,false_i,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375458951387869\n"
     ]
    }
   ],
   "source": [
    "print(\"original model acc:\")\n",
    "print(findacc(truth,preds1))\n",
    "print(\"modified model acc:\")\n",
    "print(findacc(truth,preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customized analogy pair:small big short\n",
      "prediction:\n",
      "short\n",
      "\n",
      "customized analogy pair:like fancy dislike\n",
      "prediction:\n",
      "usual\n",
      "find 10 nearest neigbour of a wordenter\n",
      "prediction:(left to right from most similar to less similar)\n",
      "['possible' 'quickly' 'safely' 'immediate' 'possibly' 'complete' 'longer'\n",
      " 'usually' 'free' 'her']\n"
     ]
    }
   ],
   "source": [
    "word1='small'\n",
    "word2='big'\n",
    "word3='short'\n",
    "print(\"customized analogy pair:\"+str(word1)+\" \"+ str(word2)+\" \"+str(word3))\n",
    "xx=-embed[word1]+embed[word2]+embed[word3]\n",
    "xx=np.expand_dims(xx,0)\n",
    "s=cosineimilarity(embeddings,xx)\n",
    "pred,dist= findknn(s,1)\n",
    "print(\"prediction:\")\n",
    "print(vocab[pred][0][0])\n",
    "\n",
    "print()\n",
    "word1='like'\n",
    "word2='fancy'\n",
    "word3='dislike'\n",
    "print(\"customized analogy pair:\"+str(word1)+\" \"+ str(word2)+\" \"+str(word3))\n",
    "xx=-embed[word1]+embed[word2]+embed[word3]\n",
    "xx=np.expand_dims(xx,0)\n",
    "s=cosineimilarity(embeddings,xx)\n",
    "pred,dist= findknn(s,1)\n",
    "print(\"prediction:\")\n",
    "print(vocab[pred][0][0])\n",
    "\n",
    "word='enter'\n",
    "print(\"find 10 nearest neigbour of a word\"+str(word))\n",
    "xx=embed[word]\n",
    "xx=np.expand_dims(xx,0)\n",
    "s=cosineimilarity(embeddings,xx)\n",
    "pred,dist= findknn(s,11)\n",
    "print(\"prediction:(left to right from most similar to less similar)\")\n",
    "print(vocab[pred][:-1,:][::-1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 10 nearest neigbour of a wordenter\n",
      "running time:0.002250558798550628\n",
      "prediction:(left to right from most similar to less similar)\n",
      "['possible']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word='enter'\n",
    "print(\"find 10 nearest neigbour of a word\"+str(word))\n",
    "xx=embed[word]\n",
    "xx=np.expand_dims(xx,0)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "s=cosineimilarity(embeddings,xx)\n",
    "pred,dist= findknn(s,2)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"running time:\"+str(elapsed))\n",
    "\n",
    "print(\"prediction:(left to right from most similar to less similar)\")\n",
    "print(vocab[pred][:-1,:][::-1].flatten())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elmo vector/allennlp\n",
    "import allennlp\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "import torch\n",
    "options_file = \"elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
    "weight_file = \"elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
    "file1_trn=\"train/obama.txt\"\n",
    "file2_trn=\"train/trump.txt\"\n",
    "file1_dev=\"development/obama.txt\"\n",
    "file2_dev=\"development/trump.txt\"\n",
    "test_file=\"test/test.txt\"\n",
    "\n",
    "#precocess and load file\n",
    "f=open(file1_trn,'r',encoding=\"utf8\")\n",
    "content1=f.readlines()\n",
    "f=open(file1_dev,'r',encoding=\"utf8\")\n",
    "content2=f.readlines()\n",
    "a=[preprocess(i) for i in content1]\n",
    "b=[preprocess(i) for i in content2]\n",
    "a=np.expand_dims(a,0)\n",
    "b=np.expand_dims(b,0)\n",
    "t=np.concatenate((a, b), axis=1)\n",
    "trn1=list(t[0])\n",
    "#\n",
    "f=open(file2_trn,'r',encoding=\"utf8\")\n",
    "content1=f.readlines()\n",
    "f=open(file2_dev,'r',encoding=\"utf8\")\n",
    "content2=f.readlines()\n",
    "a=[preprocess(i) for i in content1]\n",
    "b=[preprocess(i) for i in content2]\n",
    "a=np.expand_dims(a,0)\n",
    "b=np.expand_dims(b,0)\n",
    "t=np.concatenate((a, b), axis=1)\n",
    "trn2=list(t[0])\n",
    "\n",
    "#tokenization\n",
    "obama_tokens=[word_tokenize(i) for i in trn1]\n",
    "trump_tokens=[word_tokenize(i) for i in trn2]\n",
    "f=open(test_file,'r',encoding=\"utf8\")\n",
    "content1=f.readlines()\n",
    "a=[preprocess(i) for i in content1]\n",
    "a=np.expand_dims(a,0)\n",
    "test=list(a[0])\n",
    "test_tokens=[word_tokenize(i) for i in test]\n",
    "\n",
    "def run_elmo(tokens,flag):\n",
    "    n_chunk=math.ceil((len(tokens))/100)\n",
    "\n",
    "    #size=0\n",
    "    size=0 (#total number of words in the document)\n",
    "\n",
    "    for i in range (n_chunk):\n",
    "        result=torch.zeros(1024,dtype=torch.double)\n",
    "        result1=torch.zeros(1024,dtype=torch.double) #/total number\n",
    "        chunk=tokens[i*100:(i+1)*100]\n",
    "        for j,seq_tokens in enumerate(chunk):\n",
    "              print(i,j)\n",
    "              character_ids=batch_to_ids([seq_tokens])\n",
    "              t=elmo(character_ids)\n",
    "              a=torch.sum(t['mask'],dim=1,dtype=torch.double).unsqueeze(1)\n",
    "              size+=t['elmo_representations'][1].size()[1]\n",
    "              b=torch.sum(t['elmo_representations'][1],dim=1,dtype=torch.double)\n",
    "              result1=torch.add(result1,b.squeeze(0))\n",
    "              c=torch.div(b,a).squeeze(0)\n",
    "              result=torch.add(result,c)\n",
    "        result=result.detach().numpy()\n",
    "        result1=result1.detach().numpy()\n",
    "        if(flag==1):\n",
    "            np.save('elmo/o1_%d.npy'%(i),result)\n",
    "            np.save('elmo/o2_%d.npy'%(i),result1)\n",
    "        if(flag==1):\n",
    "            np.save('elmo/t1_%d.npy'%(i),result)\n",
    "            np.save('elmo/t2_%d.npy'%(i),result1)\n",
    "        #print(size)\n",
    "    print(\"final:\")\n",
    "    print(size) #total number of words in the document\n",
    "    print('done')\n",
    "        \n",
    "run_elmo(trump_tokens)\n",
    "run_elmo(obama_tokens)\n",
    "\n",
    "#obain elmo representation of each speech vector by taking average\n",
    "a=np.zeros(1024)\n",
    "for i in range (0,31):\n",
    "    result=np.load(\"elmo/t1_\"+str(n)+'.npy')\n",
    "    a+=result\n",
    "print(a)\n",
    "        \n",
    "b=np.zeros(1024)\n",
    "for i in range (0,31):\n",
    "    result=np.load(\"elmo/o1_\"+str(n)+'.npy')\n",
    "    b+=result\n",
    "print(a)\n",
    "        \n",
    "total=0\n",
    "results=torch.zeros(1024,dtype=torch.double)\n",
    "\n",
    "#generate embedding file for text data\n",
    "print(n_chunk)\n",
    "for i in range (len(test_tokens)):\n",
    "        character_ids=batch_to_ids([test_tokens[i]])\n",
    "        t=elmo(character_ids)\n",
    "        a=torch.sum(t['mask'],dim=1,dtype=torch.double).unsqueeze(1)\n",
    "        b=torch.sum(t['elmo_representations'][1],dim=1,dtype=torch.double)\n",
    "        c=torch.div(b,a)\n",
    "        if(i!=0):\n",
    "            c=torch.cat((prev,c),dim=0)     \n",
    "        prev=c\n",
    "np.save(\"elmo3/test.npy\",c.detach().numpy())\n",
    "\n",
    "#generate embedding file for development data\n",
    "results=torch.zeros(1024,dtype=torch.double)\n",
    "x_dev=np.concatenate((obama_tokens[3000:3100],trump_tokens[3000:3100]),axis=0)\n",
    "\n",
    "for i in range (len(x_dev)):\n",
    "        print(i)\n",
    "        character_ids=batch_to_ids([x_dev[i]])\n",
    "        t=elmo(character_ids)\n",
    "        a=torch.sum(t['mask'],dim=1,dtype=torch.double).unsqueeze(1)\n",
    "        b=torch.sum(t['elmo_representations'][1],dim=1,dtype=torch.double)\n",
    "        c=torch.div(b,a)\n",
    "        if(i!=0):\n",
    "            c=torch.cat((prev,c),dim=0)     \n",
    "        prev=c\n",
    "        print(c.size())\n",
    "\n",
    "results\n",
    "np.save(\"elmo3/dev.npy\",c.detach().numpy())        \n",
    "        \n",
    "        \n",
    "trump_embedding=a/3100\n",
    "obama_embedding=b/3100\n",
    "test_embedding=np.load(\"elmo3/test.npy\")\n",
    "        \n",
    "obama_embedding=np.expand_dims(obama_embedding,axis=0)\n",
    "trump_embedding=np.expand_dims(trump_embedding,axis=0)\n",
    "\n",
    "\n",
    "pred=[]\n",
    "Id=[]\n",
    "result=[]\n",
    "for i in range(200):\n",
    "    test=np.expand_dims(test_embedding[i],axis=0)\n",
    "\n",
    "    #print(line_embedding.shape)\n",
    "    prediction=np.argmax([cosineimilarity(obama_embedding,test),cosineimilarity(trump_embedding,test)])\n",
    "    result.append((i,prediction))\n",
    "    pred.append(prediction)\n",
    "        \n",
    "labels = ['Id','Prediction']\n",
    "df = pd.DataFrame.from_records(result, columns=labels)\n",
    "df.to_csv(\"elmo.csv\",index=False,header=True,sep=\",\")\n",
    "print('done')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo vector classficaiton acc:\n",
      "0.865\n"
     ]
    }
   ],
   "source": [
    "#obain elmo representation of each speech vector by taking average\n",
    "a=np.zeros(1024)\n",
    "for i in range (0,30):\n",
    "    result=np.load(\"elmo3/t1_\"+str(i)+'.npy')\n",
    "    a+=result\n",
    "\n",
    "b=np.zeros(1024)\n",
    "for i in range (0,30):\n",
    "    result=np.load(\"elmo3/o1_\"+str(i)+'.npy')\n",
    "    b+=result\n",
    "\n",
    "trump_embedding=a/3000\n",
    "obama_embedding=b/3000\n",
    "\n",
    "obama_embedding=np.expand_dims(obama_embedding,axis=0)\n",
    "trump_embedding=np.expand_dims(trump_embedding,axis=0)\n",
    "x_dev=np.load(\"elmo3/dev.npy\")\n",
    "\n",
    "pred=[]\n",
    "Id=[]\n",
    "result=[]\n",
    "\n",
    "for i in range(200):\n",
    "    dev=np.expand_dims(x_dev[i],axis=0)\n",
    "\n",
    "    #print(line_embedding.shape)\n",
    "    prediction=np.argmax([cosineimilarity(obama_embedding,dev),cosineimilarity(trump_embedding,dev)])\n",
    "    result.append((i,prediction))\n",
    "    pred.append(prediction)\n",
    "print(\"elmo vector classficaiton acc:\")\n",
    "print(findacc(y_dev,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the perplexity of trumps and obama speech in the dev sets\n",
    "loaddata(file1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BigramPerplexity() missing 1 required positional argument: 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-1550c2b80361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokennize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaptoken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBigramPerplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbigram_trump\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBigramPerplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbigram_obama\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: BigramPerplexity() missing 1 required positional argument: 'k'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred=[]\n",
    "Id=[]\n",
    "f=open(\"Assignment1_resources/test/test.txt\",'r',encoding=\"utf8\")\n",
    "content=f.readlines()\n",
    "len(content)\n",
    "for i,d in enumerate(content):\n",
    "    tokens=tokennize(d.strip())\n",
    "    tokens=maptoken(dict1,tokens)\n",
    "    prediction=np.argmax([BigramPerplexity(tokens,bigram_trump,dict2,vocab_size),BigramPerplexity(tokens,bigram_obama,dict1,vocab_size)])\n",
    "    pred.append((i,prediction))\n",
    "pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
